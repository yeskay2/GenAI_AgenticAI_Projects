{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a807409",
   "metadata": {},
   "source": [
    "# Costruire agenti AI con memoria persistente usando Cognee\n",
    "\n",
    "Questo notebook dimostra come creare agenti AI intelligenti con capacit√† di memoria sofisticate utilizzando [**cognee**](https://www.cognee.ai/) - una memoria AI open source che combina grafi di conoscenza, ricerca semantica e gestione delle sessioni per creare sistemi AI consapevoli del contesto.\n",
    "\n",
    "## üéØ Obiettivi di apprendimento\n",
    "\n",
    "Alla fine di questo tutorial, comprenderai come:\n",
    "- **Costruire grafi di conoscenza supportati da embeddings**: Trasformare testo non strutturato in conoscenza strutturata e interrogabile\n",
    "- **Implementare memoria di sessione**: Creare conversazioni multi-turno con conservazione automatica del contesto\n",
    "- **Conservare conversazioni**: Memorizzare opzionalmente interazioni importanti nella memoria a lungo termine per riferimenti futuri\n",
    "- **Interrogare usando il linguaggio naturale**: Accedere e sfruttare il contesto storico in nuove conversazioni\n",
    "- **Visualizzare la memoria**: Esplorare le relazioni nel grafo di conoscenza del tuo agente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5195e5f",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Cosa Costruirai\n",
    "\n",
    "In questo tutorial, creeremo un **Assistente di Programmazione** con memoria persistente che:\n",
    "\n",
    "### 1. **Costruzione della Base di Conoscenza**\n",
    "   - Assimila informazioni sul profilo e le competenze del programmatore\n",
    "   - Elabora i principi e le migliori pratiche della programmazione in Python\n",
    "   - Archivia le conversazioni storiche tra sviluppatori e assistenti AI\n",
    "\n",
    "### 2. **Conversazioni Consapevoli della Sessione**\n",
    "   - Mantiene il contesto tra pi√π domande nella stessa sessione\n",
    "   - Memorizza automaticamente ogni coppia domanda/risposta per un recupero efficiente\n",
    "   - Fornisce risposte coerenti e contestuali basate sulla cronologia della conversazione\n",
    "\n",
    "### 3. **Memoria a Lungo Termine**\n",
    "   - Conserva conversazioni importanti in una memoria a lungo termine\n",
    "   - Recupera ricordi rilevanti dalla base di conoscenza e dalle sessioni passate per informare nuove interazioni\n",
    "   - Costruisce una base di conoscenza in crescita che migliora nel tempo\n",
    "\n",
    "### 4. **Recupero Intelligente della Memoria**\n",
    "   - Utilizza una ricerca semantica basata su grafi per trovare informazioni rilevanti in tutta la conoscenza archiviata\n",
    "   - Filtra le ricerche per sottogruppi di dati (informazioni sul programmatore vs. principi)\n",
    "   - Combina pi√π fonti di dati per fornire risposte complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10fdf9d",
   "metadata": {},
   "source": [
    "## üìã Prerequisiti e Configurazione\n",
    "\n",
    "### Requisiti di Sistema\n",
    "\n",
    "Prima di iniziare, assicurati di avere:\n",
    "\n",
    "1. **Ambiente Python**\n",
    "   - Python 3.9 o superiore\n",
    "   - Ambiente virtuale (consigliato)\n",
    "   \n",
    "2. **Cache Redis** (Richiesto per la Gestione delle Sessioni)\n",
    "   - Redis locale: `docker run -d -p 6379:6379 redis`\n",
    "   - Oppure utilizza un servizio Redis gestito\n",
    "   \n",
    "3. **Accesso API LLM**\n",
    "   - Chiave API OpenAI o altri provider (vedi [documentazione](https://docs.cognee.ai/setup-configuration/llm-providers))\n",
    "\n",
    "4. **Configurazione del Database**\n",
    "   - Nessuna configurazione richiesta di default. Cognee utilizza database basati su file (LanceDB e Kuzu)\n",
    "   - Facoltativamente, puoi configurare Azure AI Search come archivio vettoriale (vedi [documentazione](https://github.com/topoteretes/cognee-community/tree/main/packages/vector/azureaisearch))\n",
    "\n",
    "### Configurazione dell'Ambiente\n",
    "\n",
    "Crea un file `.env` nella directory del tuo progetto con le seguenti variabili:\n",
    "\n",
    "```ini\n",
    "# LLM Configuration (Required)\n",
    "LLM_API_KEY=your-openai-api-key-here\n",
    "\n",
    "# Cache Configuration (Required for Sessions)\n",
    "CACHING=true  # Must be enabled for session history\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712f70d",
   "metadata": {},
   "source": [
    "## üèõÔ∏è Comprendere l'architettura della memoria di Cognee\n",
    "\n",
    "### Come funziona Cognee\n",
    "\n",
    "Cognee offre un sistema di memoria sofisticato che va oltre il semplice storage chiave-valore:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ      30+ data sources    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ\n",
    "            ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Dynamically evolving memory layers      ‚îÇ\n",
    "‚îÇ                                          ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ Knowledge Graph in Graph Database  ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ Embeddings in Vector Store         ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ   (e.g., Azure AI Search)          ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ                      ‚ñ≤   \n",
    "            ‚ñº                      ‚îÇ(optional)\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ     cognee     ‚îÇ(optional) ‚îÇ Cognee Session ‚îÇ\n",
    "‚îÇ    retrievers  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ     Cache      ‚îÇ\n",
    "‚îÇ                ‚îÇ           ‚îÇ    (Redis)     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚ñ≤\n",
    "        ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ          Agents          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "```\n",
    "\n",
    "### Componenti principali:\n",
    "\n",
    "1. **Knowledge Graph**: Archivia entit√†, relazioni e connessioni semantiche\n",
    "2. **Vector Embeddings**: Consente la ricerca semantica su tutte le informazioni archiviate\n",
    "3. **Session Cache**: Mantiene il contesto della conversazione all'interno e tra le sessioni\n",
    "4. **NodeSets**: Organizza i dati in categorie logiche per un recupero mirato\n",
    "\n",
    "### Tipi di memoria in questo tutorial:\n",
    "\n",
    "- **Memoria Persistente**: Archiviazione a lungo termine nel knowledge graph\n",
    "- **Memoria di Sessione**: Contesto temporaneo della conversazione nella cache Redis\n",
    "- **Memoria Semantica**: Ricerca basata sulla somiglianza vettoriale su tutti i dati\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadf87c",
   "metadata": {},
   "source": [
    "## üì¶ Installa i pacchetti richiesti\n",
    "\n",
    "Installa Cognee con supporto Redis per la gestione delle sessioni:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet \"cognee[redis]==0.4.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0048beb",
   "metadata": {},
   "source": [
    "## üîß Inizializza l'Ambiente e Carica le Librerie\n",
    "\n",
    "Assicurati che:\n",
    "1. Redis sia in esecuzione (ad esempio, tramite Docker: `docker run -d -p 6379:6379 redis`)\n",
    "2. Le variabili d'ambiente siano impostate prima di importare i moduli di cache\n",
    "3. Se necessario, riavvia il kernel ed esegui le celle in ordine\n",
    "\n",
    "La cella seguente:\n",
    "1. Caricher√† le variabili d'ambiente da `.env`\n",
    "2. Configurer√† Cognee con le impostazioni del tuo LLM\n",
    "3. Abiliter√† la cache per la gestione della sessione\n",
    "4. Verificher√† che tutti i componenti siano correttamente connessi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# cognee Configuration\n",
    "os.environ[\"LLM_API_KEY\"] = os.getenv(\"LLM_API_KEY\", None)\n",
    "os.environ[\"CACHING\"] = os.getenv(\"CACHING\", \"true\")\n",
    "\n",
    "\n",
    "import cognee\n",
    "\n",
    "print(f\"Cognee version: {cognee.__version__}\")\n",
    "print(f\"CACHING: {os.environ.get('CACHING')}\")\n",
    "print(f\"LLM_API_KEY: {os.environ.get('LLM_API_KEY')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c029e9",
   "metadata": {},
   "source": [
    "## üìÅ Configurare le Directory di Archiviazione\n",
    "\n",
    "Cognee utilizza due directory separate per le sue operazioni:\n",
    "- **Data Root**: Memorizza i documenti acquisiti e i dati elaborati\n",
    "- **System Root**: Contiene il database del grafo della conoscenza e i metadati di sistema\n",
    "\n",
    "Creeremo directory isolate per questo tutorial come segue:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0011f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('.data_storage').resolve()\n",
    "SYSTEM_ROOT = Path('.cognee_system').resolve()\n",
    "\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SYSTEM_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cognee.config.data_root_directory(str(DATA_ROOT))\n",
    "cognee.config.system_root_directory(str(SYSTEM_ROOT))\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"System root: {SYSTEM_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb31e1",
   "metadata": {},
   "source": [
    "## üßπ Reimposta Stato della Memoria\n",
    "\n",
    "Prima di iniziare a costruire il nostro sistema di memoria, assicuriamoci di partire da zero.\n",
    "\n",
    "> üí° **Suggerimento**: Puoi saltare questo passaggio se desideri conservare le memorie esistenti dai tuoi utilizzi precedenti di questo notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "await cognee.prune.prune_data()\n",
    "await cognee.prune.prune_system(metadata=True)\n",
    "print('Cleared previous Cognee state.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386bd5e",
   "metadata": {},
   "source": [
    "## üìö Parte 1: Creare la Base di Conoscenza\n",
    "\n",
    "### Fonti di Dati per il Nostro Assistente per Sviluppatori\n",
    "\n",
    "Utilizzeremo tre tipi di dati per creare una base di conoscenza completa:\n",
    "\n",
    "1. **Profilo dello Sviluppatore**: Competenze personali e background tecnico\n",
    "2. **Migliori Pratiche Python**: Lo Zen di Python con linee guida pratiche\n",
    "3. **Conversazioni Storiche**: Sessioni di domande e risposte passate tra sviluppatori e assistenti AI\n",
    "\n",
    "Questi dati diversificati permettono al nostro agente di:\n",
    "- Comprendere il contesto tecnico dell'utente\n",
    "- Applicare le migliori pratiche nelle raccomandazioni\n",
    "- Imparare dalle interazioni di successo precedenti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448056c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_intro = (\n",
    "  \"Hi, I'm an AI/Backend engineer. \"\n",
    "  \"I build FastAPI services with Pydantic, heavy asyncio/aiohttp pipelines, \"\n",
    "  \"and production testing via pytest-asyncio. \"\n",
    "  \"I've shipped low-latency APIs on AWS, Azure, and GoogleCloud.\"\n",
    ")\n",
    "\n",
    "python_zen_principles = (\n",
    "  \"\"\"\n",
    "    # The Zen of Python: Practical Guide\n",
    "\n",
    "    ## Overview\n",
    "    Use these principles as a checklist during design, coding, and reviews.\n",
    "\n",
    "    ## Key Principles With Guidance\n",
    "\n",
    "    ### 1. Beautiful is better than ugly\n",
    "    Prefer descriptive names, clear structure, and consistent formatting.\n",
    "\n",
    "    ### 2. Explicit is better than implicit\n",
    "    Be clear about behavior, imports, and types.\n",
    "    ```python\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    def get_future_date(days_ahead: int) -> datetime:\n",
    "        return datetime.now() + timedelta(days=days_ahead)\n",
    "    ```\n",
    "\n",
    "    ### 3. Simple is better than complex\n",
    "    Choose straightforward solutions first.\n",
    "\n",
    "    ### 4. Complex is better than complicated\n",
    "    When complexity is needed, organize it with clear abstractions.\n",
    "\n",
    "    ### 5. Flat is better than nested\n",
    "    Use early returns to reduce indentation.\n",
    "\n",
    "    ## Modern Python Tie-ins\n",
    "    - Type hints reinforce explicitness\n",
    "    - Context managers enforce safe resource handling\n",
    "    - Dataclasses improve readability for data containers\n",
    "\n",
    "    ## Quick Review Checklist\n",
    "    - Is it readable and explicit?\n",
    "    - Is this the simplest working solution?\n",
    "    - Are errors explicit and logged?\n",
    "    - Are modules/namespaces used appropriately?\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "human_agent_conversations = (\n",
    "  \"\"\"\n",
    "  \"conversations\": [\n",
    "      {\n",
    "        \"id\": \"conv_001\",\n",
    "        \"timestamp\": \"2024-01-15T10:30:00Z\",\n",
    "        \"topic\": \"async/await patterns\",\n",
    "        \"user_query\": \"I'm building a web scraper that needs to handle thousands of URLs concurrently. What's the best way to structure this with asyncio?\",\n",
    "        \"assistant_response\": \"Use asyncio with aiohttp, a semaphore to cap concurrency, TCPConnector for connection pooling, context managers for session lifecycle, and robust exception handling for failed requests.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"scraper.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"async/await\", \"context_managers\", \"semaphores\", \"aiohttp\", \"error_handling\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I add retry logic for failed requests?\",\n",
    "          \"What's the best way to parse the scraped HTML content?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_002\",\n",
    "        \"timestamp\": \"2024-01-16T14:20:00Z\",\n",
    "        \"topic\": \"dataclass vs pydantic\",\n",
    "        \"user_query\": \"When should I use dataclasses vs Pydantic models? I'm building an API and need to handle user input validation.\",\n",
    "        \"assistant_response\": \"For API input/output, prefer Pydantic: it provides runtime validation, type coercion, JSON serialization, enums for roles, field constraints, and custom validators; integrates cleanly with FastAPI for automatic request validation and error reporting.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"models.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"pydantic\", \"dataclasses\", \"validation\", \"fastapi\", \"type_hints\", \"enums\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I handle nested validation with Pydantic?\",\n",
    "          \"Can I use Pydantic with SQLAlchemy models?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_003\",\n",
    "        \"timestamp\": \"2024-01-17T09:15:00Z\",\n",
    "        \"topic\": \"testing patterns\",\n",
    "        \"user_query\": \"I'm struggling with testing async code and database interactions. What's the best approach for pytest with async functions?\",\n",
    "        \"assistant_response\": \"Recommended using pytest-asyncio, async fixtures, and an isolated test database or mocks to reliably test async functions and database interactions in FastAPI.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"test_users.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"pytest\", \"async_testing\", \"fixtures\", \"mocking\", \"database_testing\", \"fastapi_testing\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I test WebSocket connections?\",\n",
    "          \"What's the best way to test database migrations?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_004\",\n",
    "        \"timestamp\": \"2024-01-18T16:45:00Z\",\n",
    "        \"topic\": \"performance optimization\",\n",
    "        \"user_query\": \"My FastAPI app is getting slow with large datasets. How can I optimize database queries and response times?\",\n",
    "        \"assistant_response\": \"Suggested optimizing database queries (indexes, pagination, selecting only needed columns), adding caching, streaming or chunked responses for large datasets, background tasks for heavy work, and monitoring to find bottlenecks.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"optimizations.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"performance_optimization\", \"caching\", \"database_optimization\", \"async_patterns\", \"monitoring\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I implement database connection pooling properly?\",\n",
    "          \"What's the best way to handle memory usage with large datasets?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_005\",\n",
    "        \"timestamp\": \"2024-01-19T11:30:00Z\",\n",
    "        \"topic\": \"error handling and logging\",\n",
    "        \"user_query\": \"I need to implement proper error handling and logging across my Python application. What's the best approach for production-ready error management?\",\n",
    "        \"assistant_response\": \"Proposed centralized error handling with custom exceptions, structured logging, FastAPI middleware or decorators, and integration points for external monitoring/alerting tools.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"error_handling.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"error_handling\", \"logging\", \"exceptions\", \"middleware\", \"decorators\", \"fastapi\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I integrate this with external monitoring tools like Sentry?\",\n",
    "          \"What's the best way to handle errors in background tasks?\"\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "      \"total_conversations\": 5,\n",
    "      \"date_range\": \"2024-01-15 to 2024-01-19\",\n",
    "      \"topics_covered\": [\n",
    "        \"async/await patterns\",\n",
    "        \"dataclass vs pydantic\",\n",
    "        \"testing patterns\",\n",
    "        \"performance optimization\",\n",
    "        \"error handling and logging\"\n",
    "      ],\n",
    "      \"code_patterns_discussed\": [\n",
    "        \"asyncio\", \"aiohttp\", \"semaphores\", \"context_managers\",\n",
    "        \"pydantic\", \"fastapi\", \"type_hints\", \"validation\",\n",
    "        \"pytest\", \"async_testing\", \"fixtures\", \"mocking\",\n",
    "        \"performance_optimization\", \"caching\", \"database_optimization\",\n",
    "        \"error_handling\", \"logging\", \"exceptions\", \"middleware\"\n",
    "      ],\n",
    "      \"difficulty_levels\": {\n",
    "        \"beginner\": 1,\n",
    "        \"intermediate\": 2,\n",
    "        \"advanced\": 2\n",
    "      }\n",
    "    }\n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f8ae7",
   "metadata": {},
   "source": [
    "## üîÑ Trasforma i dati in un grafo di conoscenza\n",
    "\n",
    "Ora trasformeremo il nostro testo grezzo in una memoria strutturata. Questo processo:\n",
    "\n",
    "1. **Aggiunge dati ai NodeSets**: Organizza le informazioni in categorie logiche\n",
    "   - `developer_data`: Profilo dello sviluppatore e conversazioni\n",
    "   - `principles_data`: Migliori pratiche e linee guida per Python\n",
    "\n",
    "2. **Esegue il Cognify Pipeline**: Estrae entit√†, relazioni e crea embeddings\n",
    "   - Identifica i concetti chiave\n",
    "   - Crea connessioni semantiche tra informazioni correlate\n",
    "   - Genera embeddings vettoriali\n",
    "\n",
    "Questo potrebbe richiedere qualche momento mentre l'LLM elabora il testo e costruisce la struttura del grafo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea749bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "await cognee.add(developer_intro, node_set=[\"developer_data\"])\n",
    "await cognee.add(human_agent_conversations, node_set=[\"developer_data\"])\n",
    "await cognee.add(python_zen_principles, node_set=[\"principles_data\"])\n",
    "\n",
    "await cognee.cognify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e816d20",
   "metadata": {},
   "source": [
    "## üìä Visualizza il Grafo della Conoscenza\n",
    "\n",
    "Esploriamo la struttura del nostro grafo della conoscenza. La visualizzazione mostra:\n",
    "- **Nodi**: Entit√† estratte dal testo (concetti, tecnologie, persone)\n",
    "- **Archi**: Relazioni e connessioni tra le entit√†\n",
    "- **Cluster**: Concetti correlati raggruppati per somiglianza semantica\n",
    "\n",
    "Apri il file HTML generato nel tuo browser per esplorare interattivamente il grafo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef436a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognee import visualize_graph\n",
    "await visualize_graph('./visualization_1.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a7365",
   "metadata": {},
   "source": [
    "## üß† Arricchisci la memoria con Memify\n",
    "\n",
    "La funzione `memify()` analizza il grafo della conoscenza e genera regole intelligenti sui dati. Questo processo:\n",
    "- Identifica schemi e migliori pratiche\n",
    "- Crea linee guida pratiche basate sul contenuto\n",
    "- Stabilisce relazioni tra diverse aree di conoscenza\n",
    "\n",
    "Queste regole aiutano l'agente a prendere decisioni pi√π informate quando risponde alle domande. Catturare una seconda visualizzazione ti permette di confrontare come il grafo si densifica una volta arricchito.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e96b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "await cognee.memify()\n",
    "\n",
    "await visualize_graph('./visualization_2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0da911",
   "metadata": {},
   "source": [
    "## üîç Parte 2: Recupero Intelligente della Memoria\n",
    "\n",
    "### Dimostrazione 1: Integrazione della Conoscenza tra Documenti\n",
    "\n",
    "Ora che il nostro grafo della conoscenza √® stato costruito, testiamo come Cognee combina informazioni da pi√π fonti per rispondere a domande complesse.\n",
    "\n",
    "La prima query dimostra:\n",
    "- **Comprensione semantica**: Trovare concetti rilevanti anche quando non sono esplicitamente menzionati\n",
    "- **Riferimenti incrociati**: Combinare il profilo dello sviluppatore con i principi di Python\n",
    "- **Ragionamento contestuale**: Applicare le migliori pratiche a implementazioni specifiche\n",
    "\n",
    "### Dimostrazione 2: Ricerca Filtrata con NodeSets\n",
    "\n",
    "La seconda query mostra come indirizzare specifici sottoinsiemi del grafo della conoscenza:\n",
    "- Utilizza il parametro `node_name` per cercare solo all'interno di `principles_data`\n",
    "- Fornisce risposte mirate da un dominio di conoscenza specifico\n",
    "- Utile quando hai bisogno di informazioni specifiche per un determinato ambito\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a77034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate cross-document knowledge retrieval from multiple data sources\n",
    "from cognee.modules.search.types import SearchType\n",
    "\n",
    "results = await cognee.search(\n",
    "    query_text=\"How does my AsyncWebScraper implementation align with Python's design principles?\",\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    ")\n",
    "print(\"Python Pattern Analysis:\", results)\n",
    "\n",
    "# demonstrate filtered search using NodeSet to query only specific subsets of memory\n",
    "from cognee.modules.engine.models.node_set import NodeSet\n",
    "results = await cognee.search(\n",
    "    query_text=\"How should variables be named?\",\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    node_type=NodeSet,\n",
    "    node_name=[\"principles_data\"],\n",
    ")\n",
    "print(\"Filtered search result:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4dece6",
   "metadata": {},
   "source": [
    "## üîê Parte 3: Configurazione della gestione delle sessioni\n",
    "\n",
    "### Abilitare la memoria delle conversazioni\n",
    "\n",
    "La gestione delle sessioni √® fondamentale per mantenere il contesto tra pi√π interazioni. Qui faremo:\n",
    "\n",
    "1. **Inizializzare il contesto utente**: Creare o recuperare un profilo utente per il tracciamento della sessione\n",
    "2. **Configurare il motore di cache**: Collegarsi a Redis per memorizzare la cronologia delle conversazioni\n",
    "3. **Abilitare le variabili di sessione**: Configurare variabili di contesto che persistono tra le query\n",
    "\n",
    "> ‚ö†Ô∏è **Importante**: Questo richiede che Redis sia in esecuzione e `CACHING=true` nel tuo ambiente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794944bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognee.modules.users.methods import get_default_user\n",
    "from cognee.context_global_variables import set_session_user_context_variable \n",
    "from cognee.infrastructure.databases.cache import get_cache_engine\n",
    "\n",
    "user = await get_default_user()\n",
    "await set_session_user_context_variable(user)\n",
    "print(f\"Using user id: {getattr(user, 'id', 'unknown')}\")\n",
    "\n",
    "cache_engine = get_cache_engine()\n",
    "if cache_engine is None:\n",
    "    raise RuntimeError('Cache engine is not available. Double-check your cache configuration.')\n",
    "print('Session cache is ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9820b",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Funzione di supporto: Visualizza la cronologia delle sessioni\n",
    "\n",
    "Questa funzione di utilit√† ci consente di esaminare la cronologia delle conversazioni memorizzata in Redis. √à utile per:\n",
    "- Debugging della gestione delle sessioni\n",
    "- Verificare che le conversazioni vengano memorizzate nella cache\n",
    "- Comprendere quale contesto √® disponibile per l'agente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc87384",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def show_history(session_id: str) -> None:\n",
    "    # Let's check the cache directly\n",
    "    cache_engine = get_cache_engine()\n",
    "    if cache_engine:\n",
    "        # Try to get history directly from cache\n",
    "        user_id = str(user.id) if hasattr(user, 'id') else None\n",
    "        if user_id:\n",
    "            history_entries = await cache_engine.get_latest_qa(user_id, session_id, last_n=10)\n",
    "            print(f\"\\nDirect cache query for user_id={user_id}, session_id={session_id}:\")\n",
    "            print(f\"Found {len(history_entries)} entries\")\n",
    "            if history_entries:\n",
    "                for i, entry in enumerate(history_entries, 1):\n",
    "                    print(f\"\\nEntry {i}:\")\n",
    "                    print(f\"  Question: {entry.get('question', 'N/A')[:100]}...\")\n",
    "                    print(f\"  Answer: {entry.get('answer', 'N/A')[:100]}...\")\n",
    "        else:\n",
    "            print(\"No user_id available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409a744",
   "metadata": {},
   "source": [
    "## Sessione 1: Laboratorio di Supporto Async ‚Äî Prima Domanda\n",
    "\n",
    "Inizia la sessione `async-support-lab` chiedendo dei modelli asyncio adatti alla telemetria per un web scraper di grandi dimensioni. Il grafico conosce gi√† asyncio, aiohttp e le pratiche di monitoraggio, quindi la risposta dovrebbe rispecchiare le conversazioni precedenti adattandosi alla nuova domanda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6179fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1 = \"async-support-lab\"\n",
    "\n",
    "result = await cognee.search(\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    query_text=\"I'm building a web scraper that hits thousands of URLs concurrently. What's a reliable asyncio pattern with telemetry?\",\n",
    "    session_id=session_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13bc8c0",
   "metadata": {},
   "source": [
    "## Ispeziona la memoria della Sessione 1 dopo il primo scambio\n",
    "\n",
    "Eseguendo `show_history(session_1)` subito dopo la domanda iniziale, si conferma che Cognee ha scritto sia il prompt che la risposta in Redis. Dovresti vedere un'unica voce con le indicazioni sulla concorrenza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "await show_history(session_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897156e8",
   "metadata": {},
   "source": [
    "## Sessione 1: Approfondimento sui Modelli di Dati\n",
    "\n",
    "Ora ci chiediamo: \"Quando dovrei scegliere dataclasses rispetto a Pydantic?\" utilizzando lo stesso ID di sessione. Cognee dovrebbe combinare i principi di Python con le precedenti conversazioni su FastAPI per fornire consigli pi√π dettagliati, dimostrando che il contesto viene mantenuto all'interno di una sessione nominata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await cognee.search(\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    query_text=\"When should I pick dataclasses versus Pydantic for this work?\",\n",
    "    session_id=session_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805b433",
   "metadata": {},
   "source": [
    "## Conferma che la cronologia della Sessione 1 contiene entrambi i turni\n",
    "\n",
    "Un'altra chiamata a `show_history(session_1)` dovrebbe mostrare due voci di domande e risposte. Questo corrisponde alla fase di \"riproduzione della memoria\" del laboratorio Mem0 e dimostra che i turni aggiuntivi estendono la stessa trascrizione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef51215",
   "metadata": {},
   "outputs": [],
   "source": [
    "await show_history(session_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686cbec",
   "metadata": {},
   "source": [
    "## Sessione 2: Thread di Revisione del Design ‚Äî Nuova Sessione\n",
    "\n",
    "Per mostrare l'isolamento tra i thread, avviamo `design-review-session` e chiediamo indicazioni per la registrazione delle revisioni degli incidenti. Anche se la base di conoscenza sottostante √® la stessa, il nuovo ID di sessione mantiene i trascritti separati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29876d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_2 = \"design-review-session\"\n",
    "\n",
    "result = await cognee.search(\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    query_text=\"We're drafting logging guidance for incident reviews. Capture the key principles please.\",\n",
    "    session_id=session_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c522c7",
   "metadata": {},
   "source": [
    "## Revisione Sessione 2 Storia\n",
    "\n",
    "`show_history(session_2)` dovrebbe elencare solo la coppia di prompt/risposta della revisione del design. Confrontalo con la Sessione 1 per evidenziare come Cognee mantenga trascrizioni indipendenti pur riutilizzando il grafo di conoscenza condiviso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "await show_history(session_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def225fb",
   "metadata": {},
   "source": [
    "## Riassunto\n",
    "\n",
    "Congratulazioni! Hai appena dotato il tuo assistente di codifica di un vero strato di memoria a lungo termine alimentato da Cognee.\n",
    "\n",
    "In questo tutorial hai preso contenuti grezzi per sviluppatori (codice, documentazione, chat) e li hai trasformati in una memoria grafica + vettoriale che il tuo agente pu√≤ cercare, analizzare e migliorare continuamente.\n",
    "\n",
    "Cosa Hai Imparato\n",
    "\n",
    "1. **Dal testo grezzo alla memoria AI**: Come Cognee acquisisce dati non strutturati e li trasforma in una memoria intelligente e ricercabile utilizzando un'architettura combinata di grafico vettoriale + knowledge graph.\n",
    "\n",
    "2. **Arricchimento del grafico con memify**: Come andare oltre la semplice creazione di un grafico e utilizzare memify per aggiungere fatti derivati e relazioni pi√π ricche al tuo grafico esistente.\n",
    "\n",
    "3. **Strategie di ricerca multiple**: Come interrogare la memoria con diversi tipi di ricerca (Q&A consapevole del grafico, completamento in stile RAG, approfondimenti, frammenti grezzi, ricerca di codice, ecc.) a seconda delle necessit√† del tuo agente.\n",
    "\n",
    "4. **Esplorazione visiva**: Come ispezionare e fare debug di ci√≤ che Cognee ha costruito utilizzando visualizzazioni grafiche e l'interfaccia utente di Cognee, per vedere effettivamente come √® strutturata la conoscenza.\n",
    "\n",
    "5. **Memoria consapevole della sessione**: Come combinare il contesto per sessione con una memoria semantica persistente, in modo che gli agenti possano ricordare tra diverse esecuzioni senza far trapelare informazioni tra gli utenti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49959443",
   "metadata": {},
   "source": [
    "## Punti Chiave\n",
    "1. Memoria come un Grafo di Conoscenza supportato da Embeddings\n",
    "\n",
    "    - **Comprensione strutturata**: Cognee combina un archivio vettoriale e un archivio grafico, rendendo i tuoi dati sia ricercabili per significato che connessi da relazioni. Cognee utilizza database basati su file di default (LanceDB per il database vettoriale, Kuzu per il database grafico).\n",
    "\n",
    "    - **Recupero consapevole delle relazioni**: Le risposte possono essere basate non solo su \"testo simile\", ma anche su come le entit√† sono correlate.\n",
    "\n",
    "    - **Memoria vivente**: Il livello di memoria si evolve, cresce e rimane interrogabile come un grafo connesso.\n",
    "\n",
    "2. Modalit√† di Ricerca e Ragionamento\n",
    "    - **Recupero ibrido**: La ricerca combina somiglianza vettoriale, struttura del grafo e ragionamento LLM, passando dalla ricerca di frammenti grezzi alla risposta a domande consapevoli del grafo.\n",
    "\n",
    "    - **Adatta la modalit√† al compito**: Usa modalit√† di completamento quando desideri risposte in linguaggio naturale, e modalit√† di frammento/riassunto/grafo quando il tuo agente ha bisogno di contesto grezzo o di guidare il proprio ragionamento.\n",
    "\n",
    "3. Agenti Personalizzati e Consapevoli della Sessione\n",
    "    - **Contesto della sessione + memoria a lungo termine**: Cognee mantiene separato il contesto \"temporaneo\" della conversazione dalla memoria a lungo termine a livello di utente o organizzazione.\n",
    "\n",
    "## Applicazioni nel Mondo Reale\n",
    "\n",
    "1. **Agenti AI Verticali**\n",
    "\n",
    "    Usa il modello di questo notebook per alimentare copiloti intelligenti per domini specifici che si basano su Cognee come nucleo per il recupero e il ragionamento:\n",
    "\n",
    "- **Copiloti per sviluppatori**: Assistenti per revisione del codice, analisi degli incidenti e architettura che attraversano codice, API, documenti di design e ticket come un unico grafo di memoria.\n",
    "\n",
    "- **Copiloti per clienti**: Agenti di supporto o successo che attingono da documenti di prodotto, FAQ, note CRM e ticket passati con recupero consapevole del grafo e risposte citate.\n",
    "\n",
    "- **Copiloti esperti interni**: Assistenti per politiche, leggi o sicurezza che ragionano su regole interconnesse, linee guida e decisioni storiche invece che su PDF isolati.\n",
    "\n",
    "    Cognee √® esplicitamente posizionato come memoria persistente e accurata per agenti AI, fornendo un grafo di conoscenza vivente che si integra dietro il tuo agente e sostituisce combinazioni ad hoc di archivi vettoriali e codice grafico personalizzato.\n",
    "\n",
    "2. **Unificare i Silos di Dati in un'unica Memoria**\n",
    "\n",
    "    Lo stesso approccio ti aiuta a costruire un livello di memoria unificato attraverso fonti disperse:\n",
    "\n",
    "- **Da silos a un unico grafo**: Inserisci dati strutturati (ad esempio, database) e non strutturati (ad esempio, documenti, chat) in un unico grafo supportato da embeddings, invece di indici separati per ogni sistema.\n",
    "\n",
    "- **Ragionamento trasversale con citazioni**: Esegui ragionamenti multi-step su tutto‚Äî\"unisci\" log, metriche e documenti tramite il grafo‚Äîe restituisci comunque risposte fondate con provenienza.\n",
    "\n",
    "- **Hub di conoscenza**: Per domini come il settore bancario o l'istruzione, Cognee √® gi√† utilizzato per unificare PDF, sistemi interni e dati delle app in un unico grafo di conoscenza con vettori, cos√¨ gli agenti possono rispondere a domande con contesto preciso e citato.\n",
    "\n",
    "## Prossimi Passi\n",
    "\n",
    "Hai implementato il ciclo di memoria principale. Ecco alcune estensioni naturali che puoi provare autonomamente (vedi [documentazione di Cognee](https://docs.cognee.ai/) per i dettagli):\n",
    "\n",
    "1. **Sperimenta con la consapevolezza temporale**: Attiva la funzione di cognizione temporale per estrarre eventi e timestamp dal testo.\n",
    "\n",
    "2. **Introduci il ragionamento basato su ontologie**: Definisci un'ontologia OWL per il tuo dominio. Usa il supporto per ontologie di Cognee affinch√© le entit√† e le relazioni estratte siano basate su quello schema, migliorando la qualit√† del grafo e le risposte specifiche del dominio.\n",
    "\n",
    "3. **Aggiungi un ciclo di feedback**: Permetti a Cognee di regolare i pesi dei bordi del grafo in base al feedback reale degli utenti, cos√¨ il recupero migliora nel tempo invece di rimanere statico.\n",
    "\n",
    "4. **Ottimizza per personalizzazione e comportamento della sessione**: Usa ID utente, tenant e dataset per dare a ogni persona o team la propria visione sul motore di memoria condiviso.\n",
    "\n",
    "5. **Espandi verso agenti pi√π complessi**: Collega Cognee a framework per agenti per costruire sistemi multi-agente che condividono tutti lo stesso livello di memoria. *Microsoft Agent Framework x plugin Cognee in arrivo presto.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Disclaimer**:  \nQuesto documento √® stato tradotto utilizzando il servizio di traduzione AI [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale umana. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "coopTranslator": {
   "original_hash": "8f66e98ab87f8d72c25e5525d85710a1",
   "translation_date": "2025-12-09T13:50:06+00:00",
   "source_file": "13-agent-memory/13-agent-memory-cognee.ipynb",
   "language_code": "it"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}