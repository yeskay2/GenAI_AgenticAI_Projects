{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a807409",
   "metadata": {},
   "source": [
    "# Aufbau von KI-Agenten mit persistentem Ged√§chtnis mithilfe von Cognee\n",
    "\n",
    "Dieses Notebook zeigt, wie man intelligente KI-Agenten mit fortschrittlichen Ged√§chtnisf√§higkeiten mithilfe von [**cognee**](https://www.cognee.ai/) erstellt ‚Äì einem Open-Source-KI-Ged√§chtnis, das Wissensgraphen, semantische Suche und Sitzungsmanagement kombiniert, um kontextbewusste KI-Systeme zu entwickeln.\n",
    "\n",
    "## üéØ Lernziele\n",
    "\n",
    "Am Ende dieses Tutorials wirst du verstehen, wie man:\n",
    "- **Wissensgraphen basierend auf Embeddings erstellt**: Unstrukturierte Texte in strukturierte, abfragbare Wissensdaten umwandelt\n",
    "- **Sitzungsged√§chtnis implementiert**: Mehrstufige Gespr√§che mit automatischer Kontextbeibehaltung erstellt\n",
    "- **Gespr√§che speichert**: Wichtige Interaktionen optional im Langzeitged√§chtnis f√ºr zuk√ºnftige Verwendungen aufbewahrt\n",
    "- **Mit nat√ºrlicher Sprache abfragt**: Historischen Kontext in neuen Gespr√§chen nutzt und darauf zugreift\n",
    "- **Ged√§chtnis visualisiert**: Die Beziehungen im Wissensgraphen deines Agenten erkundet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5195e5f",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Was Sie bauen werden\n",
    "\n",
    "In diesem Tutorial erstellen wir einen **Coding-Assistenten** mit persistentem Speicher, der:\n",
    "\n",
    "### 1. **Wissensdatenbank-Erstellung**\n",
    "   - Entwicklerprofile und Fachkenntnisse aufnimmt\n",
    "   - Prinzipien und Best Practices der Python-Programmierung verarbeitet\n",
    "   - Historische Gespr√§che zwischen Entwicklern und KI-Assistenten speichert\n",
    "\n",
    "### 2. **Sitzungsbewusste Gespr√§che**\n",
    "   - Kontext √ºber mehrere Fragen in derselben Sitzung beibeh√§lt\n",
    "   - Jedes Frage-/Antwort-Paar automatisch zwischenspeichert, um eine effiziente Abrufbarkeit zu gew√§hrleisten\n",
    "   - Koh√§rente, kontextbezogene Antworten basierend auf der Gespr√§chshistorie liefert\n",
    "\n",
    "### 3. **Langzeitged√§chtnis**\n",
    "   - Wichtige Gespr√§che in ein Langzeitged√§chtnis speichert\n",
    "   - Relevante Erinnerungen aus der Wissensdatenbank und fr√ºheren Sitzungen abruft, um neue Interaktionen zu informieren\n",
    "   - Eine wachsende Wissensdatenbank aufbaut, die sich im Laufe der Zeit verbessert\n",
    "\n",
    "### 4. **Intelligente Speicherabrufung**\n",
    "   - Graph-bewusste semantische Suche verwendet, um relevante Informationen in allen gespeicherten Daten zu finden\n",
    "   - Suchanfragen nach Datenuntergruppen filtert (Entwicklerinformationen vs. Prinzipien)\n",
    "   - Mehrere Datenquellen kombiniert, um umfassende Antworten zu liefern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10fdf9d",
   "metadata": {},
   "source": [
    "## üìã Voraussetzungen & Einrichtung\n",
    "\n",
    "### Systemanforderungen\n",
    "\n",
    "Bevor Sie beginnen, stellen Sie sicher, dass Sie Folgendes haben:\n",
    "\n",
    "1. **Python-Umgebung**\n",
    "   - Python 3.9 oder h√∂her\n",
    "   - Virtuelle Umgebung (empfohlen)\n",
    "   \n",
    "2. **Redis-Cache** (Erforderlich f√ºr Sitzungsverwaltung)\n",
    "   - Lokales Redis: `docker run -d -p 6379:6379 redis`\n",
    "   - Oder nutzen Sie einen verwalteten Redis-Dienst\n",
    "   \n",
    "3. **LLM-API-Zugriff**\n",
    "   - OpenAI-API-Schl√ºssel oder andere Anbieter (siehe [Dokumentation](https://docs.cognee.ai/setup-configuration/llm-providers))\n",
    "\n",
    "4. **Datenbankkonfiguration**\n",
    "   - Standardm√§√üig ist keine Konfiguration erforderlich. Cognee verwendet dateibasierte Datenbanken (LanceDB und Kuzu)\n",
    "   - Optional k√∂nnen Sie Azure AI Search als Vektorspeicher einrichten (siehe [Dokumentation](https://github.com/topoteretes/cognee-community/tree/main/packages/vector/azureaisearch))\n",
    "\n",
    "### Umgebungs-Konfiguration\n",
    "\n",
    "Erstellen Sie eine `.env`-Datei in Ihrem Projektverzeichnis mit den folgenden Variablen:\n",
    "\n",
    "```ini\n",
    "# LLM Configuration (Required)\n",
    "LLM_API_KEY=your-openai-api-key-here\n",
    "\n",
    "# Cache Configuration (Required for Sessions)\n",
    "CACHING=true  # Must be enabled for session history\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712f70d",
   "metadata": {},
   "source": [
    "## üèõÔ∏è Verst√§ndnis der Speicherarchitektur von Cognee\n",
    "\n",
    "### Wie Cognee funktioniert\n",
    "\n",
    "Cognee bietet ein ausgekl√ºgeltes Speichersystem, das √ºber einfache Schl√ºssel-Wert-Speicherung hinausgeht:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ      30+ data sources    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ\n",
    "            ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Dynamically evolving memory layers      ‚îÇ\n",
    "‚îÇ                                          ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ Knowledge Graph in Graph Database  ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ Embeddings in Vector Store         ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ   (e.g., Azure AI Search)          ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ                      ‚ñ≤   \n",
    "            ‚ñº                      ‚îÇ(optional)\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ     cognee     ‚îÇ(optional) ‚îÇ Cognee Session ‚îÇ\n",
    "‚îÇ    retrievers  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ     Cache      ‚îÇ\n",
    "‚îÇ                ‚îÇ           ‚îÇ    (Redis)     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚ñ≤\n",
    "        ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ          Agents          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "```\n",
    "\n",
    "### Hauptkomponenten:\n",
    "\n",
    "1. **Wissensgraph**: Speichert Entit√§ten, Beziehungen und semantische Verbindungen\n",
    "2. **Vektoreinbettungen**: Erm√∂glicht semantische Suche √ºber alle gespeicherten Informationen\n",
    "3. **Sitzungscache**: H√§lt den Gespr√§chskontext innerhalb und zwischen Sitzungen aufrecht\n",
    "4. **NodeSets**: Organisieren Daten in logische Kategorien f√ºr gezielte Abrufe\n",
    "\n",
    "### Speicherarten in diesem Tutorial:\n",
    "\n",
    "- **Persistenter Speicher**: Langzeitspeicherung im Wissensgraph\n",
    "- **Sitzungsspeicher**: Tempor√§rer Gespr√§chskontext im Redis-Cache\n",
    "- **Semantischer Speicher**: Vektorbasierte √Ñhnlichkeitssuche √ºber alle Daten hinweg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadf87c",
   "metadata": {},
   "source": [
    "## üì¶ Erforderliche Pakete installieren\n",
    "\n",
    "Installieren Sie Cognee mit Redis-Unterst√ºtzung f√ºr die Sitzungsverwaltung:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet \"cognee[redis]==0.4.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0048beb",
   "metadata": {},
   "source": [
    "## üîß Umgebung initialisieren und Bibliotheken laden\n",
    "\n",
    "Stellen Sie sicher:\n",
    "1. Redis l√§uft (z. B. √ºber Docker: `docker run -d -p 6379:6379 redis`)\n",
    "2. Umgebungsvariablen sind gesetzt, bevor Cache-Module importiert werden\n",
    "3. Falls n√∂tig, starten Sie den Kernel neu und f√ºhren Sie die Zellen der Reihe nach aus\n",
    "\n",
    "Die folgende Zelle wird:\n",
    "1. Umgebungsvariablen aus `.env` laden\n",
    "2. Cognee mit Ihren LLM-Einstellungen konfigurieren\n",
    "3. Caching f√ºr das Sitzungsmanagement aktivieren\n",
    "4. √úberpr√ºfen, ob alle Komponenten ordnungsgem√§√ü verbunden sind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# cognee Configuration\n",
    "os.environ[\"LLM_API_KEY\"] = os.getenv(\"LLM_API_KEY\", None)\n",
    "os.environ[\"CACHING\"] = os.getenv(\"CACHING\", \"true\")\n",
    "\n",
    "\n",
    "import cognee\n",
    "\n",
    "print(f\"Cognee version: {cognee.__version__}\")\n",
    "print(f\"CACHING: {os.environ.get('CACHING')}\")\n",
    "print(f\"LLM_API_KEY: {os.environ.get('LLM_API_KEY')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c029e9",
   "metadata": {},
   "source": [
    "## üìÅ Speicherverzeichnisse konfigurieren\n",
    "\n",
    "Cognee verwendet zwei separate Verzeichnisse f√ºr seine Operationen:\n",
    "- **Daten-Root**: Speichert eingelesene Dokumente und verarbeitete Daten\n",
    "- **System-Root**: Enth√§lt die Wissensgraph-Datenbank und System-Metadaten\n",
    "\n",
    "F√ºr dieses Tutorial erstellen wir isolierte Verzeichnisse wie folgt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0011f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('.data_storage').resolve()\n",
    "SYSTEM_ROOT = Path('.cognee_system').resolve()\n",
    "\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SYSTEM_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cognee.config.data_root_directory(str(DATA_ROOT))\n",
    "cognee.config.system_root_directory(str(SYSTEM_ROOT))\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"System root: {SYSTEM_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb31e1",
   "metadata": {},
   "source": [
    "## üßπ Speicherzustand zur√ºcksetzen\n",
    "\n",
    "Bevor wir mit dem Aufbau unseres Speichersystems beginnen, stellen wir sicher, dass wir mit einem sauberen Start beginnen.\n",
    "\n",
    "> üí° **Tipp**: Sie k√∂nnen diesen Schritt √ºberspringen, wenn Sie bestehende Erinnerungen aus Ihren vorherigen Durchl√§ufen beibehalten m√∂chten, wenn Sie dieses Notebook sp√§ter verwenden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "await cognee.prune.prune_data()\n",
    "await cognee.prune.prune_system(metadata=True)\n",
    "print('Cleared previous Cognee state.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386bd5e",
   "metadata": {},
   "source": [
    "## üìö Teil 1: Aufbau der Wissensdatenbank\n",
    "\n",
    "### Datenquellen f√ºr unseren Entwicklerassistenten\n",
    "\n",
    "Wir werden drei Arten von Daten einbinden, um eine umfassende Wissensdatenbank zu erstellen:\n",
    "\n",
    "1. **Entwicklerprofil**: Pers√∂nliche Expertise und technischer Hintergrund  \n",
    "2. **Python Best Practices**: Die Zen-Prinzipien von Python mit praktischen Richtlinien  \n",
    "3. **Historische Gespr√§che**: Fr√ºhere Q&A-Sitzungen zwischen Entwicklern und KI-Assistenten  \n",
    "\n",
    "Diese vielf√§ltigen Daten erm√∂glichen es unserem Agenten:  \n",
    "- Den technischen Kontext des Nutzers zu verstehen  \n",
    "- Best Practices in Empfehlungen anzuwenden  \n",
    "- Aus fr√ºheren erfolgreichen Interaktionen zu lernen  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448056c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_intro = (\n",
    "  \"Hi, I'm an AI/Backend engineer. \"\n",
    "  \"I build FastAPI services with Pydantic, heavy asyncio/aiohttp pipelines, \"\n",
    "  \"and production testing via pytest-asyncio. \"\n",
    "  \"I've shipped low-latency APIs on AWS, Azure, and GoogleCloud.\"\n",
    ")\n",
    "\n",
    "python_zen_principles = (\n",
    "  \"\"\"\n",
    "    # The Zen of Python: Practical Guide\n",
    "\n",
    "    ## Overview\n",
    "    Use these principles as a checklist during design, coding, and reviews.\n",
    "\n",
    "    ## Key Principles With Guidance\n",
    "\n",
    "    ### 1. Beautiful is better than ugly\n",
    "    Prefer descriptive names, clear structure, and consistent formatting.\n",
    "\n",
    "    ### 2. Explicit is better than implicit\n",
    "    Be clear about behavior, imports, and types.\n",
    "    ```python\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    def get_future_date(days_ahead: int) -> datetime:\n",
    "        return datetime.now() + timedelta(days=days_ahead)\n",
    "    ```\n",
    "\n",
    "    ### 3. Simple is better than complex\n",
    "    Choose straightforward solutions first.\n",
    "\n",
    "    ### 4. Complex is better than complicated\n",
    "    When complexity is needed, organize it with clear abstractions.\n",
    "\n",
    "    ### 5. Flat is better than nested\n",
    "    Use early returns to reduce indentation.\n",
    "\n",
    "    ## Modern Python Tie-ins\n",
    "    - Type hints reinforce explicitness\n",
    "    - Context managers enforce safe resource handling\n",
    "    - Dataclasses improve readability for data containers\n",
    "\n",
    "    ## Quick Review Checklist\n",
    "    - Is it readable and explicit?\n",
    "    - Is this the simplest working solution?\n",
    "    - Are errors explicit and logged?\n",
    "    - Are modules/namespaces used appropriately?\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "human_agent_conversations = (\n",
    "  \"\"\"\n",
    "  \"conversations\": [\n",
    "      {\n",
    "        \"id\": \"conv_001\",\n",
    "        \"timestamp\": \"2024-01-15T10:30:00Z\",\n",
    "        \"topic\": \"async/await patterns\",\n",
    "        \"user_query\": \"I'm building a web scraper that needs to handle thousands of URLs concurrently. What's the best way to structure this with asyncio?\",\n",
    "        \"assistant_response\": \"Use asyncio with aiohttp, a semaphore to cap concurrency, TCPConnector for connection pooling, context managers for session lifecycle, and robust exception handling for failed requests.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"scraper.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"async/await\", \"context_managers\", \"semaphores\", \"aiohttp\", \"error_handling\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I add retry logic for failed requests?\",\n",
    "          \"What's the best way to parse the scraped HTML content?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_002\",\n",
    "        \"timestamp\": \"2024-01-16T14:20:00Z\",\n",
    "        \"topic\": \"dataclass vs pydantic\",\n",
    "        \"user_query\": \"When should I use dataclasses vs Pydantic models? I'm building an API and need to handle user input validation.\",\n",
    "        \"assistant_response\": \"For API input/output, prefer Pydantic: it provides runtime validation, type coercion, JSON serialization, enums for roles, field constraints, and custom validators; integrates cleanly with FastAPI for automatic request validation and error reporting.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"models.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"pydantic\", \"dataclasses\", \"validation\", \"fastapi\", \"type_hints\", \"enums\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I handle nested validation with Pydantic?\",\n",
    "          \"Can I use Pydantic with SQLAlchemy models?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_003\",\n",
    "        \"timestamp\": \"2024-01-17T09:15:00Z\",\n",
    "        \"topic\": \"testing patterns\",\n",
    "        \"user_query\": \"I'm struggling with testing async code and database interactions. What's the best approach for pytest with async functions?\",\n",
    "        \"assistant_response\": \"Recommended using pytest-asyncio, async fixtures, and an isolated test database or mocks to reliably test async functions and database interactions in FastAPI.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"test_users.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"pytest\", \"async_testing\", \"fixtures\", \"mocking\", \"database_testing\", \"fastapi_testing\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I test WebSocket connections?\",\n",
    "          \"What's the best way to test database migrations?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_004\",\n",
    "        \"timestamp\": \"2024-01-18T16:45:00Z\",\n",
    "        \"topic\": \"performance optimization\",\n",
    "        \"user_query\": \"My FastAPI app is getting slow with large datasets. How can I optimize database queries and response times?\",\n",
    "        \"assistant_response\": \"Suggested optimizing database queries (indexes, pagination, selecting only needed columns), adding caching, streaming or chunked responses for large datasets, background tasks for heavy work, and monitoring to find bottlenecks.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"optimizations.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"performance_optimization\", \"caching\", \"database_optimization\", \"async_patterns\", \"monitoring\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I implement database connection pooling properly?\",\n",
    "          \"What's the best way to handle memory usage with large datasets?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_005\",\n",
    "        \"timestamp\": \"2024-01-19T11:30:00Z\",\n",
    "        \"topic\": \"error handling and logging\",\n",
    "        \"user_query\": \"I need to implement proper error handling and logging across my Python application. What's the best approach for production-ready error management?\",\n",
    "        \"assistant_response\": \"Proposed centralized error handling with custom exceptions, structured logging, FastAPI middleware or decorators, and integration points for external monitoring/alerting tools.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"error_handling.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"error_handling\", \"logging\", \"exceptions\", \"middleware\", \"decorators\", \"fastapi\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I integrate this with external monitoring tools like Sentry?\",\n",
    "          \"What's the best way to handle errors in background tasks?\"\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "      \"total_conversations\": 5,\n",
    "      \"date_range\": \"2024-01-15 to 2024-01-19\",\n",
    "      \"topics_covered\": [\n",
    "        \"async/await patterns\",\n",
    "        \"dataclass vs pydantic\",\n",
    "        \"testing patterns\",\n",
    "        \"performance optimization\",\n",
    "        \"error handling and logging\"\n",
    "      ],\n",
    "      \"code_patterns_discussed\": [\n",
    "        \"asyncio\", \"aiohttp\", \"semaphores\", \"context_managers\",\n",
    "        \"pydantic\", \"fastapi\", \"type_hints\", \"validation\",\n",
    "        \"pytest\", \"async_testing\", \"fixtures\", \"mocking\",\n",
    "        \"performance_optimization\", \"caching\", \"database_optimization\",\n",
    "        \"error_handling\", \"logging\", \"exceptions\", \"middleware\"\n",
    "      ],\n",
    "      \"difficulty_levels\": {\n",
    "        \"beginner\": 1,\n",
    "        \"intermediate\": 2,\n",
    "        \"advanced\": 2\n",
    "      }\n",
    "    }\n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f8ae7",
   "metadata": {},
   "source": [
    "## üîÑ Daten in Wissensgraph umwandeln\n",
    "\n",
    "Jetzt werden wir unseren Rohtext in eine strukturierte Erinnerung umwandeln. Dieser Prozess:\n",
    "\n",
    "1. **F√ºgt Daten zu NodeSets hinzu**: Organisiert Informationen in logische Kategorien\n",
    "   - `developer_data`: Entwicklerprofile und Gespr√§che\n",
    "   - `principles_data`: Python-Best Practices und Richtlinien\n",
    "\n",
    "2. **F√ºhrt die Cognify-Pipeline aus**: Extrahiert Entit√§ten, Beziehungen und erstellt Embeddings\n",
    "   - Identifiziert Schl√ºsselkonzepte\n",
    "   - Erstellt semantische Verbindungen zwischen verwandten Informationen\n",
    "   - Generiert Vektor-Embeddings\n",
    "\n",
    "Dies kann einige Momente dauern, w√§hrend das LLM den Text verarbeitet und die Graphstruktur erstellt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea749bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "await cognee.add(developer_intro, node_set=[\"developer_data\"])\n",
    "await cognee.add(human_agent_conversations, node_set=[\"developer_data\"])\n",
    "await cognee.add(python_zen_principles, node_set=[\"principles_data\"])\n",
    "\n",
    "await cognee.cognify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e816d20",
   "metadata": {},
   "source": [
    "## üìä Visualisieren des Wissensgraphen\n",
    "\n",
    "Lassen Sie uns die Struktur unseres Wissensgraphen erkunden. Die Visualisierung zeigt:\n",
    "- **Knoten**: Aus dem Text extrahierte Entit√§ten (Konzepte, Technologien, Personen)\n",
    "- **Kanten**: Beziehungen und Verbindungen zwischen den Entit√§ten\n",
    "- **Cluster**: Verwandte Konzepte, gruppiert nach semantischer √Ñhnlichkeit\n",
    "\n",
    "√ñffnen Sie die generierte HTML-Datei in Ihrem Browser, um den Graphen interaktiv zu erkunden:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef436a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognee import visualize_graph\n",
    "await visualize_graph('./visualization_1.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a7365",
   "metadata": {},
   "source": [
    "## üß† Ged√§chtnis mit Memify bereichern\n",
    "\n",
    "Die Funktion `memify()` analysiert den Wissensgraphen und generiert intelligente Regeln √ºber die Daten. Dieser Prozess:\n",
    "- Identifiziert Muster und bew√§hrte Praktiken\n",
    "- Erstellt umsetzbare Richtlinien basierend auf dem Inhalt\n",
    "- Stellt Beziehungen zwischen verschiedenen Wissensbereichen her\n",
    "\n",
    "Diese Regeln helfen dem Agenten, fundiertere Entscheidungen zu treffen, wenn Fragen beantwortet werden. Das Erfassen einer zweiten Visualisierung erm√∂glicht es Ihnen, zu vergleichen, wie der Graph nach der Anreicherung dichter wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e96b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "await cognee.memify()\n",
    "\n",
    "await visualize_graph('./visualization_2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0da911",
   "metadata": {},
   "source": [
    "## üîç Teil 2: Intelligente Speicherabfrage\n",
    "\n",
    "### Demonstration 1: Wissensintegration √ºber Dokumente hinweg\n",
    "\n",
    "Jetzt, da unser Wissensgraph erstellt ist, testen wir, wie Cognee Informationen aus mehreren Quellen kombiniert, um komplexe Fragen zu beantworten.\n",
    "\n",
    "Die erste Abfrage demonstriert:\n",
    "- **Semantisches Verst√§ndnis**: Relevante Konzepte finden, auch wenn sie nicht explizit erw√§hnt werden\n",
    "- **Querverweise**: Verkn√ºpfung von Entwicklerprofilen mit Python-Prinzipien\n",
    "- **Kontextuelles Denken**: Anwendung von Best Practices auf spezifische Implementierungen\n",
    "\n",
    "### Demonstration 2: Gefilterte Suche mit NodeSets\n",
    "\n",
    "Die zweite Abfrage zeigt, wie spezifische Teilmengen des Wissensgraphen gezielt durchsucht werden k√∂nnen:\n",
    "- Verwendet den Parameter `node_name`, um nur innerhalb von `principles_data` zu suchen\n",
    "- Liefert fokussierte Antworten aus einem spezifischen Wissensbereich\n",
    "- N√ºtzlich, wenn dom√§nenspezifische Informationen ben√∂tigt werden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a77034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate cross-document knowledge retrieval from multiple data sources\n",
    "from cognee.modules.search.types import SearchType\n",
    "\n",
    "results = await cognee.search(\n",
    "    query_text=\"How does my AsyncWebScraper implementation align with Python's design principles?\",\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    ")\n",
    "print(\"Python Pattern Analysis:\", results)\n",
    "\n",
    "# demonstrate filtered search using NodeSet to query only specific subsets of memory\n",
    "from cognee.modules.engine.models.node_set import NodeSet\n",
    "results = await cognee.search(\n",
    "    query_text=\"How should variables be named?\",\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    node_type=NodeSet,\n",
    "    node_name=[\"principles_data\"],\n",
    ")\n",
    "print(\"Filtered search result:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4dece6",
   "metadata": {},
   "source": [
    "## üîê Teil 3: Einrichtung des Sitzungsmanagements\n",
    "\n",
    "### Aktivieren der Gespr√§chsspeicherung\n",
    "\n",
    "Das Sitzungsmanagement ist entscheidend, um den Kontext √ºber mehrere Interaktionen hinweg beizubehalten. Hier werden wir:\n",
    "\n",
    "1. **Benutzerkontext initialisieren**: Ein Benutzerprofil erstellen oder abrufen, um die Sitzung zu verfolgen\n",
    "2. **Cache-Engine konfigurieren**: Verbindung zu Redis herstellen, um den Gespr√§chsverlauf zu speichern\n",
    "3. **Sitzungsvariablen aktivieren**: Kontextvariablen einrichten, die √ºber Abfragen hinweg bestehen bleiben\n",
    "\n",
    "> ‚ö†Ô∏è **Wichtig**: Daf√ºr muss Redis laufen und `CACHING=true` in Ihrer Umgebung gesetzt sein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794944bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognee.modules.users.methods import get_default_user\n",
    "from cognee.context_global_variables import set_session_user_context_variable \n",
    "from cognee.infrastructure.databases.cache import get_cache_engine\n",
    "\n",
    "user = await get_default_user()\n",
    "await set_session_user_context_variable(user)\n",
    "print(f\"Using user id: {getattr(user, 'id', 'unknown')}\")\n",
    "\n",
    "cache_engine = get_cache_engine()\n",
    "if cache_engine is None:\n",
    "    raise RuntimeError('Cache engine is not available. Double-check your cache configuration.')\n",
    "print('Session cache is ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9820b",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Hilfsfunktion: Sitzungsverlauf anzeigen\n",
    "\n",
    "Diese Hilfsfunktion erm√∂glicht es uns, den in Redis gespeicherten Gespr√§chsverlauf zu √ºberpr√ºfen. Sie ist n√ºtzlich f√ºr:\n",
    "- Debugging der Sitzungsverwaltung\n",
    "- √úberpr√ºfung, ob Gespr√§che zwischengespeichert werden\n",
    "- Verst√§ndnis, welcher Kontext dem Agenten zur Verf√ºgung steht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc87384",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def show_history(session_id: str) -> None:\n",
    "    # Let's check the cache directly\n",
    "    cache_engine = get_cache_engine()\n",
    "    if cache_engine:\n",
    "        # Try to get history directly from cache\n",
    "        user_id = str(user.id) if hasattr(user, 'id') else None\n",
    "        if user_id:\n",
    "            history_entries = await cache_engine.get_latest_qa(user_id, session_id, last_n=10)\n",
    "            print(f\"\\nDirect cache query for user_id={user_id}, session_id={session_id}:\")\n",
    "            print(f\"Found {len(history_entries)} entries\")\n",
    "            if history_entries:\n",
    "                for i, entry in enumerate(history_entries, 1):\n",
    "                    print(f\"\\nEntry {i}:\")\n",
    "                    print(f\"  Question: {entry.get('question', 'N/A')[:100]}...\")\n",
    "                    print(f\"  Answer: {entry.get('answer', 'N/A')[:100]}...\")\n",
    "        else:\n",
    "            print(\"No user_id available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409a744",
   "metadata": {},
   "source": [
    "## Sitzung 1: Async Support Lab ‚Äî Erste Frage\n",
    "\n",
    "Starten Sie die `async-support-lab`-Sitzung, indem Sie nach telemetrie-freundlichen asyncio-Mustern f√ºr einen massiven Web-Scraper fragen. Der Graph kennt bereits asyncio, aiohttp und √úberwachungspraktiken, daher sollte die Antwort fr√ºhere Gespr√§che widerspiegeln und gleichzeitig auf die neue Anfrage zugeschnitten sein.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6179fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1 = \"async-support-lab\"\n",
    "\n",
    "result = await cognee.search(\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    query_text=\"I'm building a web scraper that hits thousands of URLs concurrently. What's a reliable asyncio pattern with telemetry?\",\n",
    "    session_id=session_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13bc8c0",
   "metadata": {},
   "source": [
    "## Speicher von Sitzung 1 nach dem ersten Austausch √ºberpr√ºfen\n",
    "\n",
    "Das Ausf√ºhren von `show_history(session_1)` direkt nach der ersten Frage best√§tigt, dass Cognee sowohl die Eingabeaufforderung als auch die Antwort in Redis geschrieben hat. Sie sollten einen Eintrag mit der Parallelit√§tsanleitung sehen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "await show_history(session_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897156e8",
   "metadata": {},
   "source": [
    "## Sitzung 1: Nachverfolgung von Datenmodellen\n",
    "\n",
    "Als N√§chstes fragen wir: \"Wann sollte ich dataclasses gegen√ºber Pydantic w√§hlen?\" unter Verwendung derselben Sitzungs-ID. Cognee sollte die Python-Prinzipien sowie fr√ºhere FastAPI-Gespr√§che zusammenf√ºhren, um differenzierte Ratschl√§ge zu geben ‚Äì und damit zeigen, dass der Kontext innerhalb einer benannten Sitzung erhalten bleibt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await cognee.search(\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    query_text=\"When should I pick dataclasses versus Pydantic for this work?\",\n",
    "    session_id=session_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805b433",
   "metadata": {},
   "source": [
    "## Best√§tigen, dass der Verlauf von Sitzung 1 beide Dialoge enth√§lt\n",
    "\n",
    "Ein weiterer Aufruf von `show_history(session_1)` sollte zwei Q&A-Eintr√§ge anzeigen. Dies entspricht dem \"Memory Replay\"-Schritt des Mem0-Labors und beweist, dass zus√§tzliche Dialoge das gleiche Transkript erweitern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef51215",
   "metadata": {},
   "outputs": [],
   "source": [
    "await show_history(session_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686cbec",
   "metadata": {},
   "source": [
    "## Sitzung 2: Design-Review-Thread ‚Äî Neue Sitzung\n",
    "\n",
    "Um die Trennung zwischen Threads zu zeigen, starten wir `design-review-session` und bitten um Protokollierungsrichtlinien f√ºr Vorfallbewertungen. Obwohl die zugrunde liegende Wissensbasis dieselbe ist, sorgt die neue Sitzungs-ID daf√ºr, dass Transkripte getrennt bleiben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29876d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_2 = \"design-review-session\"\n",
    "\n",
    "result = await cognee.search(\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    query_text=\"We're drafting logging guidance for incident reviews. Capture the key principles please.\",\n",
    "    session_id=session_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c522c7",
   "metadata": {},
   "source": [
    "## √úberpr√ºfungssitzung 2 Geschichte\n",
    "\n",
    "`show_history(session_2)` sollte nur das Design-Review-Prompt/Antwort-Paar auflisten. Vergleichen Sie es mit Sitzung 1, um hervorzuheben, wie Cognee unabh√§ngige Transkripte beibeh√§lt, w√§hrend das gemeinsame Wissensgraphen genutzt wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "await show_history(session_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def225fb",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Herzlichen Gl√ºckwunsch! Sie haben Ihrem Coding-Assistenten gerade eine echte Langzeitspeicherschicht mit Cognee hinzugef√ºgt.\n",
    "\n",
    "In diesem Tutorial haben Sie rohe Entwicklerinhalte (Code, Dokumentationen, Chats) in ein Graph- und Vektorspeicherformat umgewandelt, das Ihr Agent durchsuchen, analysieren und kontinuierlich verbessern kann.\n",
    "\n",
    "Was Sie gelernt haben:\n",
    "\n",
    "1. **Von rohem Text zu KI-Speicher**: Wie Cognee unstrukturierte Daten aufnimmt und sie mithilfe einer kombinierten Vektor- und Wissensgraph-Architektur in intelligenten, durchsuchbaren Speicher umwandelt.\n",
    "\n",
    "2. **Graph-Anreicherung mit memify**: Wie Sie √ºber die grundlegende Graph-Erstellung hinausgehen und memify nutzen k√∂nnen, um abgeleitete Fakten und reichhaltigere Beziehungen zu Ihrem bestehenden Graphen hinzuzuf√ºgen.\n",
    "\n",
    "3. **Verschiedene Suchstrategien**: Wie Sie den Speicher mit unterschiedlichen Suchtypen abfragen k√∂nnen (graph-basiertes Q&A, RAG-√§hnliche Vervollst√§ndigung, Einblicke, rohe Textabschnitte, Codesuche usw.), je nachdem, was Ihr Agent ben√∂tigt.\n",
    "\n",
    "4. **Visuelle Erkundung**: Wie Sie mit Graph-Visualisierungen und der Cognee-Benutzeroberfl√§che inspizieren und debuggen k√∂nnen, was Cognee erstellt hat, sodass Sie tats√§chlich sehen k√∂nnen, wie Wissen strukturiert ist.\n",
    "\n",
    "5. **Sitzungsbewusster Speicher**: Wie Sie kontextbezogene Sitzungsdaten mit persistentem semantischem Speicher kombinieren k√∂nnen, damit Agenten sich √ºber mehrere Sitzungen hinweg erinnern k√∂nnen, ohne Informationen zwischen Benutzern zu vermischen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49959443",
   "metadata": {},
   "source": [
    "## Wichtige Erkenntnisse\n",
    "1. Speicher als Wissensgraph unterst√ºtzt durch Embeddings\n",
    "\n",
    "    - **Strukturierte Verst√§ndlichkeit**: Cognee kombiniert einen Vektorspeicher und einen Graphspeicher, sodass Ihre Daten sowohl nach Bedeutung durchsuchbar als auch durch Beziehungen verbunden sind. Standardm√§√üig verwendet Cognee dateibasierte Datenbanken (LanceDB f√ºr Vektoren, Kuzu f√ºr Graphdatenbanken).\n",
    "\n",
    "    - **Beziehungsbewusste Suche**: Antworten basieren nicht nur auf ‚Äû√§hnlichem Text‚Äú, sondern auch darauf, wie Entit√§ten miteinander in Beziehung stehen.\n",
    "\n",
    "    - **Lebendiger Speicher**: Die Speicherebene entwickelt sich weiter, w√§chst und bleibt als ein verbundener Graph abfragbar.\n",
    "\n",
    "2. Such- und Denkmodi\n",
    "    - **Hybride Suche**: Die Suche kombiniert Vektorsimilarit√§t, Graphstruktur und LLM-Logik, von der Suche nach Rohdaten bis hin zu graphbewussten Frage-Antworten.\n",
    "\n",
    "    - **Den Modus an die Aufgabe anpassen**: Verwenden Sie Modi im Stil von Vervollst√§ndigungen, wenn Sie Antworten in nat√ºrlicher Sprache m√∂chten, und Chunk-/Zusammenfassungs-/Graph-Modi, wenn Ihr Agent Rohkontext ben√∂tigt oder eigene Schlussfolgerungen ziehen soll.\n",
    "\n",
    "3. Personalisierte, sitzungsbewusste Agenten\n",
    "    - **Sitzungskontext + Langzeitspeicher**: Cognee trennt den kurzfristigen ‚ÄûThread‚Äú-Kontext vom langfristigen Speicher auf Benutzer- oder Organisationsebene.\n",
    "\n",
    "## Anwendungen in der Praxis\n",
    "\n",
    "1. **Vertikale KI-Agenten**\n",
    "\n",
    "    Nutzen Sie das Muster aus diesem Notebook, um dom√§nenspezifische Copiloten zu entwickeln, die Cognee als Kern f√ºr Abruf und Logik verwenden:\n",
    "\n",
    "- **Entwickler-Copiloten**: Code-Review, Vorfallanalyse und Architekturassistenten, die Code, APIs, Entwurfsdokumente und Tickets als einen einzigen Speichergraphen durchlaufen.\n",
    "\n",
    "- **Kundenorientierte Copiloten**: Support- oder Erfolgsagenten, die Produktdokumente, FAQs, CRM-Notizen und fr√ºhere Tickets mit graphbewusster Suche und zitierten Antworten nutzen.\n",
    "\n",
    "- **Interne Experten-Copiloten**: Richtlinien-, Rechts- oder Sicherheitsassistenten, die √ºber miteinander verbundene Regeln, Richtlinien und historische Entscheidungen nachdenken, anstatt isolierte PDFs zu verwenden.\n",
    "\n",
    "    Cognee ist explizit als persistenter, genauer Speicher f√ºr KI-Agenten positioniert und bietet einen lebendigen Wissensgraphen, der hinter Ihrem Agenten eingesetzt wird und ad-hoc Kombinationen aus Vektorspeichern und benutzerdefiniertem Graphcode ersetzt.\n",
    "\n",
    "2. **Daten-Silos in einen Speicher vereinen**\n",
    "\n",
    "    Der gleiche Ansatz hilft Ihnen, eine einheitliche Speicherebene √ºber verstreute Quellen hinweg aufzubauen:\n",
    "\n",
    "- **Von Silos zu einem Graphen**: Strukturierte (z. B. Datenbanken) und unstrukturierte Daten (z. B. Dokumente, Chats) in einen einzigen Graphen mit Embeddings einf√ºgen, anstatt separate Indizes f√ºr jedes System zu verwenden.\n",
    "\n",
    "- **Quellen√ºbergreifende Logik mit Zitaten**: F√ºhren Sie mehrstufige Logik √ºber alles aus ‚Äì ‚Äûverbinden‚Äú Sie Protokolle, Metriken und Dokumente √ºber den Graphen ‚Äì und liefern Sie dennoch fundierte Antworten mit Herkunftsnachweisen.\n",
    "\n",
    "- **Wissenszentren**: F√ºr Bereiche wie Banken oder Bildung wird Cognee bereits genutzt, um PDFs, interne Systeme und App-Daten in einen Wissensgraphen mit Vektoren zu vereinen, sodass Agenten Fragen mit pr√§zisem, zitiertem Kontext beantworten k√∂nnen.\n",
    "\n",
    "## N√§chste Schritte\n",
    "\n",
    "Sie haben die Kernspeicherschleife implementiert. Hier sind nat√ºrliche Erweiterungen, die Sie selbst ausprobieren k√∂nnen (siehe [Cognee-Dokumentation](https://docs.cognee.ai/) f√ºr Details):\n",
    "\n",
    "1. **Mit zeitlicher Bewusstheit experimentieren**: Aktivieren Sie ‚Äûtemporal cognify‚Äú, um Ereignisse und Zeitstempel aus Text zu extrahieren.\n",
    "\n",
    "2. **Ontologie-gesteuerte Logik einf√ºhren**: Definieren Sie eine OWL-Ontologie f√ºr Ihre Dom√§ne. Nutzen Sie Cognee‚Äôs Ontologie-Unterst√ºtzung, damit extrahierte Entit√§ten und Beziehungen in diesem Schema verankert sind, was die Graphqualit√§t und dom√§nenspezifische Antworten verbessert.\n",
    "\n",
    "3. **Eine Feedback-Schleife hinzuf√ºgen**: Lassen Sie Cognee die Gewichtung von Graphkanten basierend auf echtem Benutzerfeedback anpassen, sodass die Suche sich im Laufe der Zeit verbessert, anstatt statisch zu bleiben.\n",
    "\n",
    "4. **F√ºr Personalisierung & Sitzungsverhalten optimieren**: Verwenden Sie Benutzer-IDs, Mandanten und Datens√§tze, um jeder Person oder jedem Team eine eigene Ansicht √ºber die gemeinsame Speicher-Engine zu geben.\n",
    "\n",
    "5. **Auf komplexere Agenten skalieren**: Integrieren Sie Cognee in Agenten-Frameworks, um Multi-Agenten-Systeme zu entwickeln, die alle die gleiche Speicherebene teilen. *Microsoft Agent Framework x Cognee Plugin kommt bald.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Haftungsausschluss**:  \nDieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "coopTranslator": {
   "original_hash": "8f66e98ab87f8d72c25e5525d85710a1",
   "translation_date": "2025-12-09T13:17:01+00:00",
   "source_file": "13-agent-memory/13-agent-memory-cognee.ipynb",
   "language_code": "de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}