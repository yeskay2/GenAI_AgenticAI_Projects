{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a807409",
   "metadata": {},
   "source": [
    "# Building AI Agents with Persistent Memory using Cognee\n",
    "\n",
    "This notebook demonstrates how to build intelligent AI agents with sophisticated memory capabilities using [**cognee**](https://www.cognee.ai/) - an open source AI memory that combines knowledge graphs, semantic search, and session management to create context-aware AI systems.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you'll understand how to:\n",
    "- **Build Knowledge Graphs Backed by Embeddings**: Transform unstructured text into structured, queryable knowledge\n",
    "- **Implement Session Memory**: Create multi-turn conversations with automatic context retention\n",
    "- **Persist Conversations**: Optionally store important interactions in long-term memory for future reference\n",
    "- **Query Using Natural Language**: Access and leverage historical context in new conversations\n",
    "- **Visualize Memory**: Explore the relationships in your agent's knowledge graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5195e5f",
   "metadata": {},
   "source": [
    "## üèóÔ∏è What You'll Build\n",
    "\n",
    "In this tutorial, we'll create a **Coding Assistant** with persistent memory that:\n",
    "\n",
    "### 1. **Knowledge Base Construction**\n",
    "   - Ingests developer profile and expertise information\n",
    "   - Processes Python programming principles and best practices\n",
    "   - Stores historical conversations between developers and AI assistants\n",
    "\n",
    "### 2. **Session-Aware Conversations**\n",
    "   - Maintains context across multiple questions in the same session\n",
    "   - Automatically caches each question/answer pair for efficient retrieval\n",
    "   - Provides coherent, contextual responses based on conversation history\n",
    "\n",
    "### 3. **Long-term Memory**\n",
    "   - Persists important conversations into a long-term memory\n",
    "   - Retrieves relevant memories from knowledge base and past sessions to inform new interactions\n",
    "   - Builds a growing knowledge base that improves over time\n",
    "\n",
    "### 4. **Intelligent Memory Retrieval**\n",
    "   - Uses graph-aware semantic search to find relevant information across all stored knowledge\n",
    "   - Filters searches by data subgroups (developer info vs. principles)\n",
    "   - Combines multiple data sources to provide comprehensive answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10fdf9d",
   "metadata": {},
   "source": [
    "## üìã Prerequisites & Setup\n",
    "\n",
    "### System Requirements\n",
    "\n",
    "Before starting, ensure you have:\n",
    "\n",
    "1. **Python Environment**\n",
    "   - Python 3.9 or higher\n",
    "   - Virtual environment (recommended)\n",
    "   \n",
    "2. **Redis Cache** (Required for Session Management)\n",
    "   - Local Redis: `docker run -d -p 6379:6379 redis`\n",
    "   - Or use a managed Redis service\n",
    "   \n",
    "3. **LLM API Access**\n",
    "   - OpenAI API key or other providers (see [documentation](https://docs.cognee.ai/setup-configuration/llm-providers))\n",
    "\n",
    "4. **Database Configuration**\n",
    "   - No configuration required by default. Cognee uses file-based databases (LanceDB and Kuzu)\n",
    "   - Optionally, you can setup Azure AI Search as a vectore store (see [documentation](https://github.com/topoteretes/cognee-community/tree/main/packages/vector/azureaisearch))\n",
    "\n",
    "### Environment Configuration\n",
    "\n",
    "Create a `.env` file in your project directory with the following variables:\n",
    "\n",
    "```ini\n",
    "# LLM Configuration (Required)\n",
    "LLM_API_KEY=your-openai-api-key-here\n",
    "\n",
    "# Cache Configuration (Required for Sessions)\n",
    "CACHING=true  # Must be enabled for session history\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712f70d",
   "metadata": {},
   "source": [
    "\n",
    "## üèõÔ∏è Understanding Cognee's Memory Architecture\n",
    "\n",
    "### How Cognee Works\n",
    "\n",
    "Cognee provides a sophisticated memory system that goes beyond simple key-value storage:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ      30+ data sources    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ\n",
    "            ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Dynamically evolving memory layers      ‚îÇ\n",
    "‚îÇ                                          ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ Knowledge Graph in Graph Database  ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ Embeddings in Vector Store         ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ   (e.g., Azure AI Search)          ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ                      ‚ñ≤   \n",
    "            ‚ñº                      ‚îÇ(optional)\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ     cognee     ‚îÇ(optional) ‚îÇ Cognee Session ‚îÇ\n",
    "‚îÇ    retrievers  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ     Cache      ‚îÇ\n",
    "‚îÇ                ‚îÇ           ‚îÇ    (Redis)     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚ñ≤\n",
    "        ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ          Agents          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Knowledge Graph**: Stores entities, relationships, and semantic connections\n",
    "2. **Vector Embeddings**: Enables semantic search across all stored information\n",
    "3. **Session Cache**: Maintains conversation context within and across sessions\n",
    "4. **NodeSets**: Organize data into logical categories for targeted retrieval\n",
    "\n",
    "### Memory Types in This Tutorial:\n",
    "\n",
    "- **Persistent Memory**: Long-term storage in the knowledge graph\n",
    "- **Session Memory**: Temporary conversation context in Redis cache\n",
    "- **Semantic Memory**: Vector-based similarity search across all data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadf87c",
   "metadata": {},
   "source": [
    "## üì¶ Install Required Packages\n",
    "\n",
    "Install Cognee with Redis support for session management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet \"cognee[redis]==0.4.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0048beb",
   "metadata": {},
   "source": [
    "## üîß Initialize Environment and Load Libraries\n",
    "\n",
    "Make sure:\n",
    "1. Redis is running (e.g., via Docker: `docker run -d -p 6379:6379 redis`)\n",
    "2. Environment variables are set before importing cache modules\n",
    "3. If needed, restart the kernel and run cells in order\n",
    "\n",
    "The following cell will:\n",
    "1. Load environment variables from `.env`\n",
    "2. Configure Cognee with your LLM settings\n",
    "3. Enable caching for session management\n",
    "4. Validate all components are properly connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# cognee Configuration\n",
    "os.environ[\"LLM_API_KEY\"] = os.getenv(\"LLM_API_KEY\", None)\n",
    "os.environ[\"CACHING\"] = os.getenv(\"CACHING\", \"true\")\n",
    "\n",
    "\n",
    "import cognee\n",
    "\n",
    "print(f\"Cognee version: {cognee.__version__}\")\n",
    "print(f\"CACHING: {os.environ.get('CACHING')}\")\n",
    "print(f\"LLM_API_KEY: {os.environ.get('LLM_API_KEY')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c029e9",
   "metadata": {},
   "source": [
    "## üìÅ Configure Storage Directories\n",
    "\n",
    "Cognee uses two separate directories for its operations:\n",
    "- **Data Root**: Stores ingested documents and processed data\n",
    "- **System Root**: Contains the knowledge graph database and system metadata\n",
    "\n",
    "We'll create isolated directories for this tutorial as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0011f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('.data_storage').resolve()\n",
    "SYSTEM_ROOT = Path('.cognee_system').resolve()\n",
    "\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SYSTEM_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cognee.config.data_root_directory(str(DATA_ROOT))\n",
    "cognee.config.system_root_directory(str(SYSTEM_ROOT))\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"System root: {SYSTEM_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb31e1",
   "metadata": {},
   "source": [
    "## üßπ Reset Memory State\n",
    "\n",
    "Before we begin building our memory system, let's ensure we're starting fresh.\n",
    "\n",
    "> üí° **Tip**: You can skip this step if you want to preserve existing memories from your previous runs when you use this notebook later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "await cognee.prune.prune_data()\n",
    "await cognee.prune.prune_system(metadata=True)\n",
    "print('Cleared previous Cognee state.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4386bd5e",
   "metadata": {},
   "source": [
    "## üìö Part 1: Building the Knowledge Base\n",
    "\n",
    "### Data Sources for Our Developer Assistant\n",
    "\n",
    "We'll ingest three types of data to create a comprehensive knowledge base:\n",
    "\n",
    "1. **Developer Profile**: Personal expertise and technical background\n",
    "2. **Python Best Practices**: The Zen of Python with practical guidelines\n",
    "3. **Historical Conversations**: Past Q&A sessions between developers and AI assistants\n",
    "\n",
    "This diverse data allows our agent to:\n",
    "- Understand the user's technical context\n",
    "- Apply best practices in recommendations\n",
    "- Learn from previous successful interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448056c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_intro = (\n",
    "  \"Hi, I'm an AI/Backend engineer. \"\n",
    "  \"I build FastAPI services with Pydantic, heavy asyncio/aiohttp pipelines, \"\n",
    "  \"and production testing via pytest-asyncio. \"\n",
    "  \"I've shipped low-latency APIs on AWS, Azure, and GoogleCloud.\"\n",
    ")\n",
    "\n",
    "python_zen_principles = (\n",
    "  \"\"\"\n",
    "    # The Zen of Python: Practical Guide\n",
    "\n",
    "    ## Overview\n",
    "    Use these principles as a checklist during design, coding, and reviews.\n",
    "\n",
    "    ## Key Principles With Guidance\n",
    "\n",
    "    ### 1. Beautiful is better than ugly\n",
    "    Prefer descriptive names, clear structure, and consistent formatting.\n",
    "\n",
    "    ### 2. Explicit is better than implicit\n",
    "    Be clear about behavior, imports, and types.\n",
    "    ```python\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    def get_future_date(days_ahead: int) -> datetime:\n",
    "        return datetime.now() + timedelta(days=days_ahead)\n",
    "    ```\n",
    "\n",
    "    ### 3. Simple is better than complex\n",
    "    Choose straightforward solutions first.\n",
    "\n",
    "    ### 4. Complex is better than complicated\n",
    "    When complexity is needed, organize it with clear abstractions.\n",
    "\n",
    "    ### 5. Flat is better than nested\n",
    "    Use early returns to reduce indentation.\n",
    "\n",
    "    ## Modern Python Tie-ins\n",
    "    - Type hints reinforce explicitness\n",
    "    - Context managers enforce safe resource handling\n",
    "    - Dataclasses improve readability for data containers\n",
    "\n",
    "    ## Quick Review Checklist\n",
    "    - Is it readable and explicit?\n",
    "    - Is this the simplest working solution?\n",
    "    - Are errors explicit and logged?\n",
    "    - Are modules/namespaces used appropriately?\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "human_agent_conversations = (\n",
    "  \"\"\"\n",
    "  \"conversations\": [\n",
    "      {\n",
    "        \"id\": \"conv_001\",\n",
    "        \"timestamp\": \"2024-01-15T10:30:00Z\",\n",
    "        \"topic\": \"async/await patterns\",\n",
    "        \"user_query\": \"I'm building a web scraper that needs to handle thousands of URLs concurrently. What's the best way to structure this with asyncio?\",\n",
    "        \"assistant_response\": \"Use asyncio with aiohttp, a semaphore to cap concurrency, TCPConnector for connection pooling, context managers for session lifecycle, and robust exception handling for failed requests.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"scraper.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"async/await\", \"context_managers\", \"semaphores\", \"aiohttp\", \"error_handling\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I add retry logic for failed requests?\",\n",
    "          \"What's the best way to parse the scraped HTML content?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_002\",\n",
    "        \"timestamp\": \"2024-01-16T14:20:00Z\",\n",
    "        \"topic\": \"dataclass vs pydantic\",\n",
    "        \"user_query\": \"When should I use dataclasses vs Pydantic models? I'm building an API and need to handle user input validation.\",\n",
    "        \"assistant_response\": \"For API input/output, prefer Pydantic: it provides runtime validation, type coercion, JSON serialization, enums for roles, field constraints, and custom validators; integrates cleanly with FastAPI for automatic request validation and error reporting.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"models.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"pydantic\", \"dataclasses\", \"validation\", \"fastapi\", \"type_hints\", \"enums\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I handle nested validation with Pydantic?\",\n",
    "          \"Can I use Pydantic with SQLAlchemy models?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_003\",\n",
    "        \"timestamp\": \"2024-01-17T09:15:00Z\",\n",
    "        \"topic\": \"testing patterns\",\n",
    "        \"user_query\": \"I'm struggling with testing async code and database interactions. What's the best approach for pytest with async functions?\",\n",
    "        \"assistant_response\": \"Recommended using pytest-asyncio, async fixtures, and an isolated test database or mocks to reliably test async functions and database interactions in FastAPI.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"test_users.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"pytest\", \"async_testing\", \"fixtures\", \"mocking\", \"database_testing\", \"fastapi_testing\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I test WebSocket connections?\",\n",
    "          \"What's the best way to test database migrations?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_004\",\n",
    "        \"timestamp\": \"2024-01-18T16:45:00Z\",\n",
    "        \"topic\": \"performance optimization\",\n",
    "        \"user_query\": \"My FastAPI app is getting slow with large datasets. How can I optimize database queries and response times?\",\n",
    "        \"assistant_response\": \"Suggested optimizing database queries (indexes, pagination, selecting only needed columns), adding caching, streaming or chunked responses for large datasets, background tasks for heavy work, and monitoring to find bottlenecks.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"optimizations.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"performance_optimization\", \"caching\", \"database_optimization\", \"async_patterns\", \"monitoring\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I implement database connection pooling properly?\",\n",
    "          \"What's the best way to handle memory usage with large datasets?\"\n",
    "        ]\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"conv_005\",\n",
    "        \"timestamp\": \"2024-01-19T11:30:00Z\",\n",
    "        \"topic\": \"error handling and logging\",\n",
    "        \"user_query\": \"I need to implement proper error handling and logging across my Python application. What's the best approach for production-ready error management?\",\n",
    "        \"assistant_response\": \"Proposed centralized error handling with custom exceptions, structured logging, FastAPI middleware or decorators, and integration points for external monitoring/alerting tools.\",\n",
    "        \"code_context\": {\n",
    "          \"file\": \"error_handling.py\",\n",
    "          \"language\": \"python\",\n",
    "          \"patterns_discussed\": [\"error_handling\", \"logging\", \"exceptions\", \"middleware\", \"decorators\", \"fastapi\"]\n",
    "        },\n",
    "        \"follow_up_questions\": [\n",
    "          \"How do I integrate this with external monitoring tools like Sentry?\",\n",
    "          \"What's the best way to handle errors in background tasks?\"\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "      \"total_conversations\": 5,\n",
    "      \"date_range\": \"2024-01-15 to 2024-01-19\",\n",
    "      \"topics_covered\": [\n",
    "        \"async/await patterns\",\n",
    "        \"dataclass vs pydantic\",\n",
    "        \"testing patterns\",\n",
    "        \"performance optimization\",\n",
    "        \"error handling and logging\"\n",
    "      ],\n",
    "      \"code_patterns_discussed\": [\n",
    "        \"asyncio\", \"aiohttp\", \"semaphores\", \"context_managers\",\n",
    "        \"pydantic\", \"fastapi\", \"type_hints\", \"validation\",\n",
    "        \"pytest\", \"async_testing\", \"fixtures\", \"mocking\",\n",
    "        \"performance_optimization\", \"caching\", \"database_optimization\",\n",
    "        \"error_handling\", \"logging\", \"exceptions\", \"middleware\"\n",
    "      ],\n",
    "      \"difficulty_levels\": {\n",
    "        \"beginner\": 1,\n",
    "        \"intermediate\": 2,\n",
    "        \"advanced\": 2\n",
    "      }\n",
    "    }\n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f8ae7",
   "metadata": {},
   "source": [
    "## üîÑ Process Data into Knowledge Graph\n",
    "\n",
    "Now we'll transform our raw text into a structured memory. This process:\n",
    "\n",
    "1. **Adds data to NodeSets**: Organizes information into logical categories\n",
    "   - `developer_data`: Developer profile and conversations\n",
    "   - `principles_data`: Python best practices and guidelines\n",
    "\n",
    "2. **Runs Cognify Pipeline**: Extracts entities, relationships, and creates embeddings\n",
    "   - Identifies key concepts\n",
    "   - Creates semantic connections between related information\n",
    "   - Generates vector embeddings\n",
    "\n",
    "This may take a few moments as the LLM processes the text and builds the graph structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea749bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "await cognee.add(developer_intro, node_set=[\"developer_data\"])\n",
    "await cognee.add(human_agent_conversations, node_set=[\"developer_data\"])\n",
    "await cognee.add(python_zen_principles, node_set=[\"principles_data\"])\n",
    "\n",
    "await cognee.cognify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e816d20",
   "metadata": {},
   "source": [
    "## üìä Visualize the Knowledge Graph\n",
    "\n",
    "Let's explore the structure of our knowledge graph. The visualization shows:\n",
    "- **Nodes**: Entities extracted from the text (concepts, technologies, people)\n",
    "- **Edges**: Relationships and connections between entities\n",
    "- **Clusters**: Related concepts grouped by semantic similarity\n",
    "\n",
    "Open the generated HTML file in your browser to interactively explore the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef436a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognee import visualize_graph\n",
    "await visualize_graph('./visualization_1.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a7365",
   "metadata": {},
   "source": [
    "## üß† Enrich Memory with Memify\n",
    "\n",
    "The `memify()` function analyzes the knowledge graph and generates intelligent rules about the data. This process:\n",
    "- Identifies patterns and best practices\n",
    "- Creates actionable guidelines based on the content\n",
    "- Establishes relationships between different knowledge areas\n",
    "\n",
    "These rules help the agent make more informed decisions when answering questions. Capturing a second visualization helps you compare how the graph densifies once enriched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e96b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "await cognee.memify()\n",
    "\n",
    "await visualize_graph('./visualization_2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0da911",
   "metadata": {},
   "source": [
    "## üîç Part 2: Intelligent Memory Retrieval\n",
    "\n",
    "### Demonstration 1: Cross-Document Knowledge Integration\n",
    "\n",
    "Now that our knowledge graph is built, let's test how Cognee combines information from multiple sources to answer complex questions. \n",
    "\n",
    "The first query demonstrates:\n",
    "- **Semantic understanding**: Finding relevant concepts even when not explicitly mentioned\n",
    "- **Cross-referencing**: Combining developer profile with Python principles\n",
    "- **Contextual reasoning**: Applying best practices to specific implementations\n",
    "\n",
    "### Demonstration 2: Filtered Search with NodeSets\n",
    "\n",
    "The second query shows how to target specific subsets of the knowledge graph:\n",
    "- Uses `node_name` parameter to search only within `principles_data`\n",
    "- Provides focused answers from a specific knowledge domain\n",
    "- Useful for when you need domain-specific information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a77034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate cross-document knowledge retrieval from multiple data sources\n",
    "from cognee.modules.search.types import SearchType\n",
    "\n",
    "results = await cognee.search(\n",
    "    query_text=\"How does my AsyncWebScraper implementation align with Python's design principles?\",\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    ")\n",
    "print(\"Python Pattern Analysis:\", results)\n",
    "\n",
    "# demonstrate filtered search using NodeSet to query only specific subsets of memory\n",
    "from cognee.modules.engine.models.node_set import NodeSet\n",
    "results = await cognee.search(\n",
    "    query_text=\"How should variables be named?\",\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    node_type=NodeSet,\n",
    "    node_name=[\"principles_data\"],\n",
    ")\n",
    "print(\"Filtered search result:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4dece6",
   "metadata": {},
   "source": [
    "## üîê Part 3: Session Management Setup\n",
    "\n",
    "### Enabling Conversation Memory\n",
    "\n",
    "Session management is crucial for maintaining context across multiple interactions. Here we'll:\n",
    "\n",
    "1. **Initialize User Context**: Create or retrieve a user profile for session tracking\n",
    "2. **Configure Cache Engine**: Connect to Redis for storing conversation history\n",
    "3. **Enable Session Variables**: Set up context variables that persist across queries\n",
    "\n",
    "> ‚ö†Ô∏è **Important**: This requires Redis to be running and `CACHING=true` in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794944bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognee.modules.users.methods import get_default_user\n",
    "from cognee.context_global_variables import set_session_user_context_variable \n",
    "from cognee.infrastructure.databases.cache import get_cache_engine\n",
    "\n",
    "user = await get_default_user()\n",
    "await set_session_user_context_variable(user)\n",
    "print(f\"Using user id: {getattr(user, 'id', 'unknown')}\")\n",
    "\n",
    "cache_engine = get_cache_engine()\n",
    "if cache_engine is None:\n",
    "    raise RuntimeError('Cache engine is not available. Double-check your cache configuration.')\n",
    "print('Session cache is ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9820b",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Helper Function: View Session History\n",
    "\n",
    "This utility function allows us to inspect the conversation history stored in Redis. It's useful for:\n",
    "- Debugging session management\n",
    "- Verifying that conversations are being cached\n",
    "- Understanding what context is available to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc87384",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def show_history(session_id: str) -> None:\n",
    "    # Let's check the cache directly\n",
    "    cache_engine = get_cache_engine()\n",
    "    if cache_engine:\n",
    "        # Try to get history directly from cache\n",
    "        user_id = str(user.id) if hasattr(user, 'id') else None\n",
    "        if user_id:\n",
    "            history_entries = await cache_engine.get_latest_qa(user_id, session_id, last_n=10)\n",
    "            print(f\"\\nDirect cache query for user_id={user_id}, session_id={session_id}:\")\n",
    "            print(f\"Found {len(history_entries)} entries\")\n",
    "            if history_entries:\n",
    "                for i, entry in enumerate(history_entries, 1):\n",
    "                    print(f\"\\nEntry {i}:\")\n",
    "                    print(f\"  Question: {entry.get('question', 'N/A')[:100]}...\")\n",
    "                    print(f\"  Answer: {entry.get('answer', 'N/A')[:100]}...\")\n",
    "        else:\n",
    "            print(\"No user_id available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409a744",
   "metadata": {},
   "source": [
    "## Session 1: Async Support Lab ‚Äî First Question\n",
    "\n",
    "Kick off the `async-support-lab` session by asking for telemetry-friendly asyncio patterns for a massive web scraper. The graph already knows about asyncio, aiohttp, and monitoring practices, so the response should mirror prior conversations while tailoring the answer to the new query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6179fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1 = \"async-support-lab\"\n",
    "\n",
    "result = await cognee.search(\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    query_text=\"I'm building a web scraper that hits thousands of URLs concurrently. What's a reliable asyncio pattern with telemetry?\",\n",
    "    session_id=session_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13bc8c0",
   "metadata": {},
   "source": [
    "## Inspect Session 1 Memory After the First Exchange\n",
    "\n",
    "Running `show_history(session_1)` immediately after the initial question confirms that Cognee wrote both the prompt and completion into Redis. You should see one entry with the concurrency guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "await show_history(session_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897156e8",
   "metadata": {},
   "source": [
    "## Session 1: Follow-up on Data Models\n",
    "\n",
    "Next we ask, \"When should I pick dataclasses versus Pydantic?\" using the same session id. Cognee should stitch together the Python principles plus prior FastAPI conversations to provide nuanced advice‚Äîdemonstrating that context carries over within a named session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await cognee.search(\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    query_text=\"When should I pick dataclasses versus Pydantic for this work?\",\n",
    "    session_id=session_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805b433",
   "metadata": {},
   "source": [
    "## Confirm Session 1 History Contains Both Turns\n",
    "\n",
    "Another `show_history(session_1)` call should reveal two Q&A entries. This matches the Mem0 lab's \"memory replay\" step and proves that additional turns extend the same transcript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef51215",
   "metadata": {},
   "outputs": [],
   "source": [
    "await show_history(session_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686cbec",
   "metadata": {},
   "source": [
    "## Session 2: Design Review Thread ‚Äî Fresh Session\n",
    "\n",
    "To show isolation between threads we spin up `design-review-session` and ask for logging guidance for incident reviews. Even though the underlying knowledge base is the same, the new session id keeps transcripts separate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29876d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_2 = \"design-review-session\"\n",
    "\n",
    "result = await cognee.search(\n",
    "    query_type=SearchType.GRAPH_COMPLETION,\n",
    "    query_text=\"We're drafting logging guidance for incident reviews. Capture the key principles please.\",\n",
    "    session_id=session_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c522c7",
   "metadata": {},
   "source": [
    "## Review Session 2 History\n",
    "\n",
    "`show_history(session_2)` should only list the design-review prompt/response pair. Compare it with Session 1 to highlight how Cognee keeps independent transcripts while reusing the shared knowledge graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "await show_history(session_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def225fb",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "Congratulations! You‚Äôve just given your coding assistant a real long-term memory layer powered by Cognee.\n",
    "\n",
    "In this tutorial you took raw developer content (code, docs, chats) and turned it into a graph + vector memory that your agent can search, reason over, and continuously improve.\n",
    "\n",
    "What You‚Äôve Learned\n",
    "\n",
    "1. **From raw text to AI memory**: How Cognee ingests unstructured data and turns it into intelligent, searchable memory using a combined vector + knowledge graph architecture.\n",
    "\n",
    "2. **Graph enrichment with memify**: How to go beyond basic graph creation and use memify to add derived facts and richer relationships on top of your existing graph. \n",
    "\n",
    "3. **Multiple search strategies**: How to query memory with different search types (graph-aware Q&A, RAG-style completion, insights, raw chunks, code search, etc.) depending on what your agent needs. \n",
    "\n",
    "4. **Visual exploration**: How to inspect and debug what Cognee built using graph visualizations and the Cognee UI, so you can actually see how knowledge is structured. \n",
    "\n",
    "5. **Session-aware memory**: How to combine per-session context with persistent semantic memory so that agents can remember across runs without leaking information between users. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49959443",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "1. Memory as a Knowledge Graph backed by Embeddings\n",
    "\n",
    "    - **Structured understanding**: Cognee combines a vector store and a graph store so your data is both searchable by meaning and connected by relationships. Cognee uses file-based databases by default (LanceDB for vector-, Kuzu for graph database)\n",
    "\n",
    "    - **Relationship-aware retrieval**: Answers can be grounded not only in ‚Äúsimilar text,‚Äù but also in how entities relate.\n",
    "\n",
    "    - **Living memory**: The memory layer evolves, grows, and stays queryable as one connected graph. \n",
    "\n",
    "2. Search & Reasoning Modes\n",
    "    - **Hybrid retrieval**: search blends vector similarity, graph structure, and LLM reasoning, from raw chunk lookup to graph-aware question answering. \n",
    "\n",
    "    - **Fit the mode to the job**: Use completion-style modes when you want natural language answers, and chunk/summary/graph modes when your agent needs raw context or to drive its own reasoning.\n",
    "\n",
    "3. Personalized, Session-Aware Agents\n",
    "    - **Session context + long-term memory**: Cognee keeps short-term ‚Äúthread‚Äù context separate from long-lived, user- or org-level memory. \n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "1. **Vertical AI Agents**\n",
    "\n",
    "    Use the pattern from this notebook to power domain-smart copilots that sit on top of Cognee as their retrieval and reasoning core:\n",
    "\n",
    "- **Developer copilots**: Code review, incident analysis, and architecture assistants that traverse code, APIs, design docs, and tickets as a single memory graph.\n",
    "\n",
    "- **Customer-facing copilots**: Support or success agents that pull from product docs, FAQs, CRM notes, and past tickets with graph-aware retrieval and cited answers.\n",
    "\n",
    "- **Internal expert copilots**: Policy, legal, or security assistants that reason over interconnected rules, guidelines, and historical decisions instead of isolated PDFs.\n",
    "\n",
    "    Cognee is explicitly positioned as persistent, accurate memory for AI agents, providing a living knowledge graph that slots in behind your agent and replaces ad-hoc combinations of vector stores and custom graph code. \n",
    "\n",
    "2. **Unifying Data Silos into One Memory**\n",
    "\n",
    "    The same approach also helps you build a unified memory layer across scattered sources:\n",
    "\n",
    "- **From silos to one graph**: Ingest structured (e.g., databases) and unstructured data (e.g., docs, chats) into a single graph backed by embeddings, rather than separate indices for each system. \n",
    "\n",
    "- **Cross-source reasoning with citations**: Run multi-step reasoning over everything‚Äî‚Äújoin‚Äù logs, metrics, and docs via the graph‚Äîand still return grounded answers with provenance. \n",
    "\n",
    "- **Knowledge hubs**: For domains like banking or education, Cognee is already used to unify PDFs, internal systems, and app data into one knowledge graph with vectors so agents can answer questions with precise, cited context. \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You‚Äôve implemented the core memory loop. Here are natural extensions you can try on your own (see [Cognee documentation](https://docs.cognee.ai/) for details):\n",
    "\n",
    "1. **Experiment with temporal awareness**: Turn on temporal cognify to extract events and timestamps from text.\n",
    "\n",
    "2. **Introduce ontology-driven reasoning** Define an OWL ontology for your domain. Use Cognee‚Äôs ontology support so extracted entities and relations are grounded in that schema, improving graph quality and domain-specific answers. \n",
    "\n",
    "3. **Add a feedback loop**: Let Cognee adjust graph edge weights from real user feedback, so retrieval improves over time instead of staying static. \n",
    "\n",
    "4. **Tune for personalization & session behavior**: Use user IDs, tenants, and datasets to give each person or team their own view over the shared memory engine. \n",
    "\n",
    "5. **Scale out to more complex agents**: Plug Cognee into agent frameworks to build multi-agent systems that all share the same memory layer. *Microsoft Agent Framework x Cognee plugin is coming soon.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
