{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "19206bb2",
         "metadata": {},
         "source": [
            "![](https://europe-west1-atp-views-tracker.cloudfunctions.net/working-analytics?notebook=tutorials--agent-with-brightdata--web-scraping-agent)\n",
            "\n",
            "# Building an Intelligent Web Scraping Agent with LangGraph and Bright Data + MCP\n",
            "\n",
            "## Overview\n",
            "\n",
            "This comprehensive tutorial demonstrates how to build a production-ready web scraping agent that combines LangGraph's ReAct (Reasoning and Acting) framework with Bright Data's advanced scraping infrastructure. The resulting system can intelligently navigate websites, extract structured data, and conduct complex research workflows with minimal human intervention.\n",
            "\n",
            "The agent you'll build represents a significant advancement over traditional web scraping approaches. Instead of writing custom scrapers for each website, you'll create an intelligent system that can reason about different scraping strategies, select appropriate tools, and adapt to various web structures automatically.\n",
            "\n",
            "## Learning Objectives\n",
            "\n",
            "By completing this tutorial, you will understand how to:\n",
            "\n",
            "- Implement LangGraph ReAct agents with external tool integration for autonomous decision-making\n",
            "- Configure Bright Data's Model Context Protocol (MCP) server for enterprise-grade web scraping\n",
            "- Design intelligent agents that dynamically select optimal scraping strategies based on target websites\n",
            "- Extract structured data from major platforms including e-commerce sites, social media, and news sources\n",
            "- Implement browser automation workflows for complex user interactions\n",
            "- Build comprehensive research pipelines that synthesize information from multiple sources\n",
            "\n",
            "## Architecture Overview\n",
            "\n",
            "The system architecture consists of three primary components working in harmony:\n",
            "\n",
            "```\n",
            "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ   User Query    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   ReAct Agent    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Bright Data MCP   ‚îÇ\n",
            "‚îÇ                 ‚îÇ    ‚îÇ   (LangGraph)    ‚îÇ    ‚îÇ      Server         ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ                  ‚îÇ    ‚îÇ                     ‚îÇ\n",
            "                       ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
            "                       ‚îÇ ‚îÇ Reasoning    ‚îÇ ‚îÇ    ‚îÇ ‚îÇ Search Engines  ‚îÇ ‚îÇ\n",
            "                       ‚îÇ ‚îÇ Engine       ‚îÇ ‚îÇ    ‚îÇ ‚îÇ Web Scrapers    ‚îÇ ‚îÇ\n",
            "                       ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ ‚îÇ Platform APIs   ‚îÇ ‚îÇ\n",
            "                       ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚îÇ Browser Tools   ‚îÇ ‚îÇ\n",
            "                       ‚îÇ ‚îÇ Tool         ‚îÇ ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
            "                       ‚îÇ ‚îÇ Selection    ‚îÇ ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "                       ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
            "                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "```\n",
            "\n",
            "The ReAct agent serves as the intelligent coordinator, analyzing user requests and selecting appropriate tools from Bright Data's comprehensive suite of web scraping capabilities. This design ensures both flexibility and reliability in handling diverse web scraping scenarios."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "0fad8237",
         "metadata": {},
         "source": [
            "## Prerequisites and Environment Setup\n",
            "\n",
            "Before beginning the implementation, you'll need to establish accounts and configure your development environment. This section guides you through the essential setup steps.\n",
            "\n",
            "### Required Accounts and API Keys\n",
            "\n",
            "**Bright Data Account Setup**\n",
            "\n",
            "1. Create a Bright Data account at [This link](https://brightdata.com/?hs_signup=1&utm_source=brand&utm_campaign=brnd-mkt_github_nirdiamant_logo). New accounts receive 5,000 unlocker requests monthly at no cost, providing substantial resources for development and testing.\n",
            "    \n",
            "    ![Screenshot: Signup Page](assets/Signup.png)\n",
            "\n",
            "2. Navigate to your [account settings](https://brightdata.com/cp/setting) and locate your API key. This credential will authenticate your agent with Bright Data's infrastructure.\n",
            "\n",
            "    ![Screenshot: Settings Page](assets/Settings.png)\n",
            "\n",
            "**Language Model Access**\n",
            "\n",
            "3. Register for an OpenRouter account to obtain API access for language models. While this tutorial uses Gemini through OpenRouter for optimal performance and cost efficiency, the architecture supports any compatible language model.\n",
            "\n",
            "**Development Environment**\n",
            "\n",
            "4. Ensure your development environment includes Python 3.8 or higher with pip package management capabilities."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "d6aa62c5",
         "metadata": {},
         "source": [
            "## Dependency Installation and Configuration\n",
            "\n",
            "The following section installs the required Python packages and establishes the foundational imports for our web scraping agent. Each dependency serves a specific role in the overall architecture."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "587dc1bf",
         "metadata": {},
         "source": [
            "### API Key Configuration\n",
            "\n",
            "This cell creates environment variables for your API credentials. Replace the placeholder values with your actual API keys before execution."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "da3690af",
         "metadata": {},
         "outputs": [],
         "source": [
            "# To export your API key into a .env file, run the following cell (replace with your actual keys):\n",
            "!echo \"BRIGHT_DATA_API_TOKEN=<your-brightdataa-api-key>\" >> .env\n",
            "!echo \"OPENROUTER_API_KEY=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\" >> .env"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "package_installation",
         "metadata": {},
         "source": [
            "### Package Installation\n",
            "\n",
            "This cell installs the core dependencies required for the web scraping agent. The packages include LangGraph for agent orchestration, OpenAI client for language model interaction, MCP client for Bright Data integration, and supporting utilities."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "54735c68",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Note: you may need to restart the kernel to use updated packages.\n"
               ]
            }
         ],
         "source": [
            "# Install required packages\n",
            "%pip install langgraph langchain-openai mcp-use python-dotenv asyncio --quiet\n",
            "%load_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "imports_section",
         "metadata": {},
         "source": [
            "### Core Imports and Environment Loading\n",
            "\n",
            "This cell imports the essential libraries and loads environment variables from the configuration file. The imports include async handling capabilities, language model clients, agent frameworks, and MCP integration components."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "6be9197b",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "True"
                  ]
               },
               "execution_count": 1,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# Import all necessary libraries\n",
            "import asyncio\n",
            "from langchain_openai import ChatOpenAI\n",
            "from langgraph.prebuilt import create_react_agent\n",
            "from mcp_use.client import MCPClient\n",
            "from mcp_use.adapters.langchain_adapter import LangChainAdapter\n",
            "from dotenv import load_dotenv\n",
            "import os\n",
            "\n",
            "# Load environment variables from .env\n",
            "load_dotenv()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "e8ca08a1",
         "metadata": {},
         "source": [
            "### Environment Verification\n",
            "\n",
            "Before proceeding with agent configuration, it's essential to verify that all required environment variables are properly loaded. This verification step prevents runtime errors and ensures smooth operation."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "88678ad6",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "‚úÖ Environment setup complete!\n",
                  "OpenRouter API Key loaded: Yes\n"
               ]
            }
         ],
         "source": [
            "# Verify environment setup\n",
            "print(\"‚úÖ Environment setup complete!\")\n",
            "print(f\"OpenRouter API Key loaded: {'Yes' if os.getenv('OPENROUTER_API_KEY') else 'No'}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "9157cd65",
         "metadata": {},
         "source": [
            "## Understanding the ReAct Agent Framework\n",
            "\n",
            "The ReAct (Reasoning and Acting) framework represents a significant advancement in autonomous agent design. Unlike traditional rule-based systems or simple chain-of-thought approaches, ReAct agents integrate reasoning capabilities with action execution in a unified framework.\n",
            "\n",
            "### Core Principles of ReAct Architecture\n",
            "\n",
            "ReAct agents operate on a fundamental principle that combines deliberative reasoning with interactive tool usage. This approach enables agents to:\n",
            "\n",
            "**Dynamic Reasoning**: The agent analyzes each situation and develops a reasoning strategy tailored to the specific task requirements. Rather than following predetermined scripts, it evaluates available information and determines the most effective approach.\n",
            "\n",
            "**Tool Selection**: Based on its reasoning, the agent selects appropriate tools from its available repertoire. This selection process considers factors such as data requirements, website characteristics, and desired output formats.\n",
            "\n",
            "**Iterative Refinement**: The agent can execute multiple actions in sequence, using the results of previous actions to inform subsequent decisions. This capability is particularly valuable for complex web scraping scenarios that require multi-step workflows.\n",
            "\n",
            "### Application to Web Scraping\n",
            "\n",
            "In the context of web scraping, ReAct agents excel because they can adapt their approach based on the target website's characteristics. For instance, when encountering an e-commerce site, the agent might recognize the need for structured product data extraction. Conversely, when analyzing news articles, it might prioritize content extraction and summarization techniques.\n",
            "\n",
            "This adaptability eliminates the need for maintaining separate scraping scripts for different websites, significantly reducing development overhead while improving reliability and maintainability."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "9ec99515",
         "metadata": {},
         "source": [
            "## Bright Data MCP Integration\n",
            "\n",
            "The Model Context Protocol (MCP) represents a standardized approach for integrating external services with language model applications. Bright Data's MCP server provides access to a comprehensive suite of web scraping tools through a unified interface.\n",
            "\n",
            "### MCP Architecture Benefits\n",
            "\n",
            "The MCP integration offers several advantages over direct API integration:\n",
            "\n",
            "**Standardized Interface**: All Bright Data tools are accessible through consistent function signatures, simplifying agent development and reducing integration complexity.\n",
            "\n",
            "**Automatic Tool Discovery**: The MCP server dynamically exposes available tools, allowing the agent to discover and utilize new capabilities without code modifications.\n",
            "\n",
            "**Error Handling**: Built-in error handling and retry mechanisms improve reliability when dealing with network issues or temporary service unavailability.\n",
            "\n",
            "**Performance Optimization**: The MCP server includes caching and request optimization features that enhance overall system performance."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "mcp_setup_section",
         "metadata": {},
         "source": [
            "### Bright Data MCP Server Configuration\n",
            "\n",
            "This function establishes the connection to Bright Data's MCP server and converts the available tools into LangChain-compatible format. The configuration process involves setting up the server connection parameters and authenticating with your API credentials."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "520a8012",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-22 14:19:26,387 - mcp_use - INFO - No active sessions found, creating new ones...\n",
                  "‚úÖ Connected to Bright Data MCP server\n",
                  "üìä Available tools: 60\n"
               ]
            }
         ],
         "source": [
            "async def setup_bright_data_tools():\n",
            "    \"\"\"\n",
            "    Configure Bright Data MCP client and create LangChain-compatible tools\n",
            "    \"\"\"\n",
            "    \n",
            "    # Configure the MCP server connection\n",
            "    bright_data_config = {\n",
            "        \"mcpServers\": {\n",
            "                    \"Bright Data\": {\n",
            "            \"command\": \"npx\",\n",
            "            \"args\": [\"@brightdata/mcp\"],\n",
            "            \"env\": {\n",
            "                \"API_TOKEN\": os.getenv(\"BRIGHT_DATA_API_TOKEN\"),\n",
            "            }\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "    \n",
            "    # Create MCP client and adapter\n",
            "    client = MCPClient.from_dict(bright_data_config)\n",
            "    adapter = LangChainAdapter()\n",
            "    \n",
            "    # Convert MCP tools to LangChain-compatible format\n",
            "    tools = await adapter.create_tools(client)\n",
            "    \n",
            "    print(f\"‚úÖ Connected to Bright Data MCP server\")\n",
            "    print(f\"üìä Available tools: {len(tools)}\")\n",
            "    \n",
            "    return tools\n",
            "\n",
            "# Test the connection\n",
            "tools = await setup_bright_data_tools()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "available_tools_section",
         "metadata": {},
         "source": [
            "### Available Tool Categories\n",
            "\n",
            "The Bright Data MCP server provides access to several categories of web scraping tools, each optimized for specific use cases:\n",
            "\n",
            "**Search Engine Integration**: Comprehensive access to major search engines including Google, Bing, and Yandex, enabling the agent to discover relevant web content based on user queries.\n",
            "\n",
            "**Universal Web Scraping**: General-purpose scraping capabilities that can extract content from any website in multiple formats including Markdown and HTML, with built-in bot detection bypass mechanisms.\n",
            "\n",
            "**Platform-Specific Extractors**: Specialized tools for major platforms such as Amazon, LinkedIn, Instagram, Facebook, X (Twitter), TikTok, YouTube, Reddit, and Zillow. These extractors understand the specific data structures of each platform and can extract information more efficiently than general-purpose scrapers.\n",
            "\n",
            "**Browser Automation**: Advanced capabilities for simulating user interactions including navigation, clicking, typing, and screenshot capture. These tools are essential for websites that require complex user interactions or JavaScript execution.\n",
            "\n",
            "The diversity of available tools ensures that the agent can handle virtually any web scraping scenario with optimal efficiency and reliability."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "5c05dc52",
         "metadata": {},
         "source": [
            "## Language Model and Agent Configuration\n",
            "\n",
            "The next phase involves configuring the language model and creating the ReAct agent with a comprehensive system prompt. The system prompt plays a crucial role in defining the agent's behavior, reasoning patterns, and tool selection strategies."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "agent_creation_section",
         "metadata": {},
         "source": [
            "### ReAct Agent Initialization\n",
            "\n",
            "This function creates the complete web scraping agent by combining the language model with the Bright Data tools. The system prompt is carefully crafted to guide the agent's decision-making process and ensure optimal tool utilization."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "e126e5b6",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2025-07-22 14:23:30,529 - mcp_use - INFO - No active sessions found, creating new ones...\n",
                  "‚úÖ Connected to Bright Data MCP server\n",
                  "üìä Available tools: 60\n",
                  "ü§ñ ReAct Web Scraper Agent created successfully!\n"
               ]
            }
         ],
         "source": [
            "import datetime\n",
            "\n",
            "async def create_web_scraper_agent():\n",
            "    \"\"\"\n",
            "    Create a ReAct agent configured for intelligent web scraping\n",
            "    \"\"\"\n",
            "    \n",
            "    # Get the tools from Bright Data first\n",
            "    tools = await setup_bright_data_tools()\n",
            "    \n",
            "    # Get current date\n",
            "    current_date = datetime.datetime.now().strftime(\"%B %d, %Y\")\n",
            "    \n",
            "    # Initialize the language model\n",
            "    llm = ChatOpenAI(\n",
            "        openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
            "        openai_api_base=\"https://openrouter.ai/api/v1\",\n",
            "        model_name=\"google/gemini-2.5-flash-lite-preview-06-17\",  # Fast and capable model for reasoning\n",
            "        temperature=0.1  # Low temperature for consistent, focused responses\n",
            "    )\n",
            "    \n",
            "    # Define comprehensive system prompt for the agent with dynamic date and tool count\n",
            "    system_prompt = f\"\"\"You are a web data extraction agent. Today's date is {current_date}.\n",
            "\n",
            "You have {len(tools)} specialized tools for web scraping and data extraction. When users request web data or current information, you MUST use these tools - do not rely on your training data.\n",
            "\n",
            "Available capabilities:\n",
            "- Search engines (Google/Bing/Yandex)\n",
            "- Universal web scraping (any website)\n",
            "- Platform extractors (Amazon, LinkedIn, Instagram, Facebook, X, TikTok, YouTube, Reddit, etc.)\n",
            "- Browser automation\n",
            "\n",
            "Process:\n",
            "1. Identify data need\n",
            "2. Select appropriate tool\n",
            "3. Execute extraction\n",
            "4. Return structured results\n",
            "\n",
            "Always use tools for current/live data requests.\"\"\"\n",
            "    \n",
            "    # Create the ReAct agent\n",
            "    agent = create_react_agent(\n",
            "        model=llm,\n",
            "        tools=tools,\n",
            "        prompt=system_prompt\n",
            "    )\n",
            "    \n",
            "    print(\"ü§ñ ReAct Web Scraper Agent created successfully!\")\n",
            "    return agent\n",
            "\n",
            "# Create the agent\n",
            "agent = await create_web_scraper_agent()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "system_prompt_explanation",
         "metadata": {},
         "source": [
            "### System Prompt Design Principles\n",
            "\n",
            "The system prompt serves as the foundation for the agent's behavior and incorporates several key design principles:\n",
            "\n",
            "**Clear Role Definition**: The prompt establishes the agent's identity as a web data extraction specialist, providing clear context for its primary function.\n",
            "\n",
            "**Tool Awareness**: By explicitly stating the number and types of available tools, the prompt ensures the agent understands its capabilities and prioritizes tool usage over relying on training data.\n",
            "\n",
            "**Process Framework**: The four-step process outlined in the prompt provides a structured approach to handling user requests, ensuring consistent and methodical responses.\n",
            "\n",
            "**Temporal Context**: Including the current date helps the agent understand the temporal context of requests and prioritize recent information when relevant.\n",
            "\n",
            "**Mandatory Tool Usage**: The instruction to always use tools for current data requests prevents the agent from providing outdated information from its training data."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "bdebc69d",
         "metadata": {},
         "source": [
            "## Basic Search Functionality Testing\n",
            "\n",
            "With the agent configured, we can now test its fundamental capabilities. The first test demonstrates the agent's ability to search for current information and synthesize results from multiple sources.\n",
            "\n",
            "### Understanding Agent Decision-Making\n",
            "\n",
            "When presented with a search query, the agent follows a systematic decision-making process. It first analyzes the query to understand the information requirements, then selects the most appropriate search tool based on the query characteristics. The agent then executes the search, analyzes the results, and presents a structured summary."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "basic_search_test_section",
         "metadata": {},
         "source": [
            "### Basic Search Query Execution\n",
            "\n",
            "This test demonstrates the agent's ability to search for current information and process multiple search results into a coherent summary. The agent will automatically select appropriate search engines and present the findings in a structured format."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "c3d39402",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Testing Basic Search Functionality...\n",
                  "==================================================\n",
                  "\n",
                  "üîç Search Results:\n",
                  "Here's the latest AI news from this week:\n",
                  "\n",
                  "*   **Why Apple is playing it slow with AI** - Artificial Intelligence News: https://www.artificialintelligence-news.com/\n",
                  "*   **Tech giants split on EU AI code as compliance deadline looms** - Artificial Intelligence News: https://www.artificialintelligence-news.com/\n",
                  "*   **Can speed and safety truly coexist in the AI race?** - Artificial Intelligence News: https://www.artificialintelligence-news.com/\n",
                  "*   **Mistral** - Artificial Intelligence News: https://www.artificialintelligence-news.com/\n",
                  "*   **Latent Labs launches web-based AI model to democratize protein design** - TechCrunch: https://techcrunch.com/category/artificial-intelligence/\n",
                  "*   **Instead of selling to Meta, AI chip startup FuriosaAI signed a huge customer** - TechCrunch: https://techcrunch.com/category/artificial-intelligence/\n",
                  "*   **OpenAI and** - TechCrunch: https://techcrunch.com/category/artificial-intelligence/\n",
                  "*   **Nvidia CEO Draws Rock-Star Reception in China After Chip Sales OK'd** - The Wall Street Journal: https://www.wsj.com/tech/ai\n",
                  "*   **Trump Touts Billions in AI Investments from Blackstone, Google** - The Wall Street Journal: https://www.wsj.com/tech/ai\n",
                  "*   **The Real Energy Cost of AI** - The Wall Street Journal: https://www.wsj.com/tech/ai\n",
                  "*   **Subscribe to a weekly collection of AI News and resources on Artificial Intelligence and Machine Learning. For free.** - AI Weekly: https://aiweekly.co/\n",
                  "*   **Microsoft server hack hit about 100 organizations, experts say** - Reuters: https://www.reuters.com/technology/artificial-intelligence/\n",
                  "*   **Researchers say AI-powered medical imaging tech could cut radiation exposure** - Reuters: https://www.reuters.com/technology/artificial-intelligence/\n",
                  "*   **Meta investors,** - Reuters: https://www.reuters.com/technology/artificial-intelligence/\n",
                  "*   **Leaked Memo: Anthropic CEO Says the Company Will Pursue Gulf State Investments After All** - WIRED: https://www.wired.com/tag/artificial-intelligence/\n",
                  "*   **OpenAI's New CEO of Applications Strikes** - WIRED: https://www.wired.com/tag/artificial-intelligence/\n",
                  "*   **Netflix uses AI effects for first time to cut costs‚Äã‚Äã** - BBC News: https://www.bbc.com/news/topics/ce1qrvleleqt\n",
                  "*   **The streaming firm says AI allowed The Eternaut to complete a sequence faster and cheaper.** - BBC News: https://www.bbc.com/news/topics/ce1qrvleleqt\n",
                  "*   **How OpenAI is Seeing the UK to AI Glory via Data Centres** - AI Magazine: https://aimagazine.com/\n",
                  "*   **Intelliscale: Meeting AI's Rising Demands in Data Centres** - AI Magazine: https://aimagazine.com/\n",
                  "*   **Hexagon: The AI Robotics Transformation in** - AI Magazine: https://aimagazine.com/\n",
                  "*   **European Union Unveils Rules for Powerful A.I. Systems** - The New York Times: https://www.nytimes.com/spotlight/artificial-intelligence\n",
                  "*   **A.I.-Generated Images of Child Sexual Abuse Are Flooding the Internet. Organizations that track the** - The New York Times: https://www.nytimes.com/spotlight/artificial-intelligence\n",
                  "*   **The latest news and top stories on artificial intelligence, including AI chatbots like Microsoft's ChatGPT, Apple's AI Chatbot and Google's Bard.** - NBC News: https://www.nbcnews.com/artificial-intelligence\n"
               ]
            }
         ],
         "source": [
            "async def test_basic_search():\n",
            "    \"\"\"\n",
            "    Test the agent's ability to search for current information\n",
            "    \"\"\"\n",
            "    \n",
            "    print(\"Testing Basic Search Functionality...\")\n",
            "    print(\"=\"*50)\n",
            "    \n",
            "    # Simple search query\n",
            "    search_result = await agent.ainvoke({\n",
            "        \"messages\": [(\"human\", \"Give me the latest AI news from this week, Include full URLs to source.\")],\n",
            "    })\n",
            "    \n",
            "    print(\"\\nüîç Search Results:\")\n",
            "    print(search_result[\"messages\"][-1].content)\n",
            "    \n",
            "    return search_result\n",
            "\n",
            "# Execute the test\n",
            "basic_search_result = await test_basic_search()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "search_analysis_section",
         "metadata": {},
         "source": [
            "### Search Result Analysis\n",
            "\n",
            "The search functionality demonstrates several key capabilities of the ReAct agent:\n",
            "\n",
            "**Query Understanding**: The agent correctly interprets the request for recent AI news and understands the temporal requirement (\"this week\").\n",
            "\n",
            "**Tool Selection**: Based on the query type, the agent selects the appropriate search engine tool from its available options.\n",
            "\n",
            "**Result Processing**: The agent aggregates results from multiple sources and presents them in a structured, readable format with source URLs.\n",
            "\n",
            "**Content Prioritization**: The results show the agent's ability to identify and prioritize relevant, current information from authoritative sources."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2fc39bb7",
         "metadata": {},
         "source": [
            "## Advanced Platform-Specific Data Extraction\n",
            "\n",
            "Beyond general web searching, the agent's true power lies in its ability to extract structured data from specific platforms. Each platform presents unique challenges in terms of data structure, access methods, and content organization.\n",
            "\n",
            "### E-commerce Platform Analysis\n",
            "\n",
            "E-commerce platforms like Amazon contain rich structured data including product specifications, pricing, reviews, and availability information. The agent's platform-specific tools can navigate these complex data structures and extract relevant information efficiently.\n",
            "\n",
            "When analyzing e-commerce data, the agent considers multiple factors including product features, pricing trends, customer feedback, and comparative analysis across similar products. This comprehensive approach provides users with actionable insights for purchase decisions or market research."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ecommerce_test_section",
         "metadata": {},
         "source": [
            "### E-commerce Data Extraction and Analysis\n",
            "\n",
            "This test demonstrates the agent's ability to research and compare products on e-commerce platforms. The agent will search for products, extract structured data, and provide comparative analysis with pricing and feature information."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "707e4dff",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Testing E-commerce Data Extraction...\n",
                  "==================================================\n",
                  "\n",
                  "üõí E-commerce Analysis:\n",
                  "I found several top-rated wireless headphones on Amazon. Here's a comparison of some of the most popular options:\n",
                  "\n",
                  "**1. Sony WH-1000XM4**\n",
                  "*   **Features:** Industry-leading noise cancellation, excellent sound quality, comfortable design, long battery life (up to 30 hours), speak-to-chat functionality, multipoint connection.\n",
                  "*   **Price:** Typically around $350, but often on sale.\n",
                  "\n",
                  "**2. Bose QuietComfort 45**\n",
                  "*   **Features:** Renowned noise cancellation, comfortable for long wear, balanced sound, good battery life (up to 24 hours), Aware Mode for hearing surroundings.\n",
                  "*   **Price:** Around $330, with occasional discounts.\n",
                  "\n",
                  "**3. Apple AirPods Max**\n",
                  "*   **Features:** Premium build quality, excellent active noise cancellation, immersive spatial audio, seamless integration with Apple devices, comfortable fit.\n",
                  "*   **Price:** Premium pricing, usually around $550.\n",
                  "\n",
                  "**4. Sennheiser Momentum 4 Wireless**\n",
                  "*   **Features:** Exceptional sound quality, very long battery life (up to 60 hours), effective noise cancellation, comfortable design, customizable EQ.\n",
                  "*   **Price:** Around $380.\n",
                  "\n",
                  "**5. JBL Tune 510BT**\n",
                  "*   **Features:** Affordable option, decent sound quality, up to 40 hours of battery life, lightweight and foldable design, built-in microphone.\n",
                  "*   **Price:** Typically under $50.\n",
                  "\n",
                  "**6. Anker Soundcore Life Q20**\n",
                  "*   **Features:** Hybrid active noise cancellation, Hi-Fi stereo sound, deep bass, up to 40 hours of playtime, comfortable earcups.\n",
                  "*   **Price:** Usually around $60.\n",
                  "\n",
                  "**To help you choose, consider these factors:**\n",
                  "\n",
                  "*   **Budget:** The JBL Tune 510BT and Anker Soundcore Life Q20 are great budget-friendly options. The Sony, Bose, Apple, and Sennheiser models are in the premium price range.\n",
                  "*   **Noise Cancellation:** Sony WH-1000XM4 and Bose QuietComfort 45 are top contenders for the best noise cancellation.\n",
                  "*   **Sound Quality:** Sony, Bose, Sennheiser, and Apple are generally praised for their superior audio performance.\n",
                  "*   **Battery Life:** Sennheiser Momentum 4 Wireless leads with up to 60 hours, while Sony and JBL offer very competitive battery life as well.\n",
                  "*   **Ecosystem:** If you're heavily invested in the Apple ecosystem, AirPods Max offer the most seamless integration.\n",
                  "\n",
                  "I recommend checking the specific product pages on Amazon for the most up-to-date pricing and detailed feature comparisons. Would you like me to look up any of these models in more detail?\n"
               ]
            }
         ],
         "source": [
            "async def test_ecommerce_scraping():\n",
            "    \"\"\"\n",
            "    Test structured data extraction from e-commerce platforms\n",
            "    \"\"\"\n",
            "    \n",
            "    print(\"Testing E-commerce Data Extraction...\")\n",
            "    print(\"=\"*50)\n",
            "    \n",
            "    # Ask the agent to find and analyze a product\n",
            "    ecommerce_result = await agent.ainvoke({\n",
            "        \"messages\": [(\"human\", \"Find information about the top-rated wireless headphones on Amazon and compare their features and prices\")]\n",
            "    })\n",
            "    \n",
            "    print(\"\\nüõí E-commerce Analysis:\")\n",
            "    print(ecommerce_result[\"messages\"][-1].content)\n",
            "    \n",
            "    return ecommerce_result\n",
            "\n",
            "# Execute the test\n",
            "ecommerce_result = await test_ecommerce_scraping()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "ecommerce_capabilities_section",
         "metadata": {},
         "source": [
            "### E-commerce Analysis Capabilities\n",
            "\n",
            "The e-commerce analysis demonstrates several sophisticated capabilities:\n",
            "\n",
            "**Product Discovery**: The agent can search for products within specific categories and identify top-rated options based on customer reviews and ratings.\n",
            "\n",
            "**Feature Extraction**: Detailed product specifications, features, and technical details are systematically extracted and organized for easy comparison.\n",
            "\n",
            "**Price Analysis**: Current pricing information is gathered and presented alongside historical pricing trends when available.\n",
            "\n",
            "**Comparative Framework**: The agent structures its analysis to facilitate decision-making by highlighting key differentiators between products.\n",
            "\n",
            "**Actionable Recommendations**: Rather than simply presenting data, the agent provides guidance based on different use cases and budget considerations."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "cbc0ae66",
         "metadata": {},
         "source": [
            "## Social Media Content Analysis\n",
            "\n",
            "Social media platforms represent a unique challenge for data extraction due to their dynamic nature, complex user interfaces, and diverse content formats. The agent's social media capabilities enable analysis of discussions, sentiment, and trending topics across various platforms.\n",
            "\n",
            "### Reddit Discussion Analysis\n",
            "\n",
            "Reddit's structure of communities (subreddits) and threaded discussions provides rich opportunities for understanding public opinion and expert insights on specific topics. The agent can navigate Reddit's interface, extract relevant discussions, and analyze the content for key themes and insights."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "social_media_test_section",
         "metadata": {},
         "source": [
            "### Reddit Content Extraction and Analysis\n",
            "\n",
            "This test demonstrates the agent's ability to search for and analyze social media content. The agent will search for relevant Reddit discussions and extract key conversation topics and community insights."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "id": "350e0e28",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Testing Reddit Extraction...\n",
                  "==================================================\n",
                  "\n",
                  "üì± Reddit Analysis:\n",
                  "Here are some recent discussions from the r/electricvehicles subreddit:\n",
                  "\n",
                  "*   **\"This $11,000 Chinese EV with semi-solid state batteries is about to shake up the industry\"**: This post discusses a new, affordable EV from China that features semi-solid state batteries, potentially disrupting the market.\n",
                  "*   **\"Anyone else want an EV just so you no longer have to deal with the bullshit that comes with owning an ICE engine?\"**: This user expresses their desire for an EV to avoid the common maintenance and repair issues associated with internal combustion engine (ICE) vehicles, highlighting the peace of mind EVs offer.\n",
                  "*   **\"These cars are losing value fast ‚Äî that's GREAT news for used EV buyers!\"**: This article suggests that the rapid depreciation of some EVs is creating good opportunities for those looking to buy used electric vehicles.\n",
                  "\n",
                  "It seems the community is actively discussing new EV technology, the benefits of EV ownership over traditional cars, and the used EV market.\n"
               ]
            }
         ],
         "source": [
            "async def test_social_media_simple():\n",
            "    \"\"\"\n",
            "    Test Reddit extraction with a specific approach\n",
            "    \"\"\"\n",
            "    print(\"Testing Reddit Extraction...\")\n",
            "    print(\"=\"*50)\n",
            "    \n",
            "    result = await agent.ainvoke({\n",
            "        \"messages\": [(\"human\", \"Search for 'electric vehicles reddit' and then scrape one of the Reddit discussion pages you find. Show me what people are discussing.\")]\n",
            "    })\n",
            "    \n",
            "    print(\"\\nüì± Reddit Analysis:\")\n",
            "    print(result[\"messages\"][-1].content)\n",
            "    return result\n",
            "\n",
            "# Test this version\n",
            "social_simple = await test_social_media_simple()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "social_media_insights_section",
         "metadata": {},
         "source": [
            "### Social Media Analysis Insights\n",
            "\n",
            "The social media analysis showcases several key capabilities:\n",
            "\n",
            "**Platform-Specific Navigation**: The agent understands Reddit's structure and can navigate to specific subreddits and discussion threads.\n",
            "\n",
            "**Content Aggregation**: Multiple posts and comments are analyzed to identify common themes and discussion topics.\n",
            "\n",
            "**Sentiment Analysis**: The agent can interpret the tone and sentiment of discussions, identifying positive, negative, or neutral attitudes toward topics.\n",
            "\n",
            "**Trend Identification**: By analyzing multiple discussions, the agent can identify emerging trends and popular topics within communities.\n",
            "\n",
            "**Context Understanding**: The agent maintains context about the specific community culture and discussion norms when interpreting content."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "7722d2b1",
         "metadata": {},
         "source": [
            "## Complex Multi-Step Research Workflows\n",
            "\n",
            "The agent's most sophisticated capability lies in conducting complex research that requires multiple tools, reasoning steps, and information synthesis. These workflows demonstrate the full potential of the ReAct framework in handling real-world research scenarios.\n",
            "\n",
            "### Research Workflow Architecture\n",
            "\n",
            "Complex research workflows involve several interconnected phases:\n",
            "\n",
            "**Task Decomposition**: The agent analyzes complex queries and breaks them down into manageable sub-tasks that can be addressed individually.\n",
            "\n",
            "**Tool Orchestration**: Multiple tools are employed in sequence, with each tool's output informing the selection and configuration of subsequent tools.\n",
            "\n",
            "**Information Synthesis**: Data from various sources is analyzed, compared, and synthesized into coherent insights that address the original research question.\n",
            "\n",
            "**Quality Validation**: The agent evaluates the quality and relevance of gathered information, identifying gaps that require additional research."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "complex_research_test_section",
         "metadata": {},
         "source": [
            "### Multi-Step Research Execution\n",
            "\n",
            "This test challenges the agent with a complex research task requiring multiple information sources and analysis steps. The agent must demonstrate its ability to plan research steps, execute them systematically, and synthesize findings into actionable insights."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "id": "72dbeb60",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Testing Complex Multi-Step Research...\n",
                  "==================================================\n",
                  "\n",
                  "üî¨ Complex Research Results:\n",
                  "I can help you research the renewable energy market. However, I need some clarification to proceed effectively.\n",
                  "\n",
                  "For step 1, could you please specify what you mean by \"recent news\"? For example, are you interested in news from the past week, month, or a specific event?\n",
                  "\n",
                  "For step 2, which major renewable energy companies are you interested in? Knowing specific company names will help me find their stock performance more accurately.\n",
                  "\n",
                  "For step 3, analyzing social media sentiment requires access to specific platforms and potentially specialized tools. Could you clarify which social media platforms you'd like me to analyze (e.g., X, Reddit, etc.)?\n",
                  "\n",
                  "Once I have this information, I can use my tools to gather the data you need for a comprehensive market overview.\n"
               ]
            }
         ],
         "source": [
            "async def test_complex_research():\n",
            "    \"\"\"\n",
            "    Test the agent's ability to conduct multi-step research\n",
            "    \"\"\"\n",
            "    \n",
            "    print(\"Testing Complex Multi-Step Research...\")\n",
            "    print(\"=\"*50)\n",
            "    \n",
            "    # Complex research query\n",
            "    research_result = await agent.ainvoke({\n",
            "        \"messages\": [(\"human\", \"\"\"\n",
            "        I need to research the current state of the renewable energy market. Please:\n",
            "        1. Find recent news about renewable energy developments\n",
            "        2. Look up major renewable energy companies and their stock performance\n",
            "        3. Analyze social media sentiment about renewable energy\n",
            "        4. Provide a comprehensive market overview with key insights\n",
            "        \"\"\")]\n",
            "    })\n",
            "    \n",
            "    print(\"\\nüî¨ Complex Research Results:\")\n",
            "    print(research_result[\"messages\"][-1].content)\n",
            "    \n",
            "    return research_result\n",
            "\n",
            "# Execute the test\n",
            "research_result = await test_complex_research()"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "research_methodology_section",
         "metadata": {},
         "source": [
            "### Research Methodology and Planning\n",
            "\n",
            "The agent's response to complex research requests demonstrates sophisticated planning capabilities:\n",
            "\n",
            "**Requirement Analysis**: The agent evaluates the research request and identifies areas where additional specification would improve research quality.\n",
            "\n",
            "**Scope Definition**: Rather than proceeding with assumptions, the agent seeks clarification to ensure research efforts are focused and relevant.\n",
            "\n",
            "**Tool Mapping**: The agent understands which tools are appropriate for different types of information gathering and outlines its approach.\n",
            "\n",
            "**Quality Assurance**: By requesting specific parameters, the agent ensures that the final research output will meet professional standards and user expectations.\n",
            "\n",
            "This planning approach reflects best practices in professional research methodology and demonstrates the agent's capability to handle enterprise-level research requirements."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "b2e6727a",
         "metadata": {},
         "source": [
            "## Comprehensive Research Assistant Implementation\n",
            "\n",
            "The final component of our tutorial involves creating a comprehensive research assistant function that demonstrates the full integration of all the agent's capabilities. This function serves as a template for building production-ready research workflows.\n",
            "\n",
            "### Research Assistant Architecture\n",
            "\n",
            "The research assistant function incorporates several design principles that make it suitable for production use:\n",
            "\n",
            "**Parameterized Configuration**: Users can specify research scope, source limits, and other parameters to control the research process.\n",
            "\n",
            "**Structured Output**: Research results are organized in a consistent format that facilitates further analysis or reporting.\n",
            "\n",
            "**Progress Tracking**: The function provides feedback on research progress, helping users understand the complexity and scope of the work being performed.\n",
            "\n",
            "**Quality Controls**: Built-in mechanisms ensure that research focuses on high-quality, relevant sources rather than simply maximizing the quantity of information gathered."
         ]
      },
      {
         "cell_type": "markdown",
         "id": "research_assistant_implementation_section",
         "metadata": {},
         "source": [
            "### Research Assistant Function Implementation\n",
            "\n",
            "This comprehensive function demonstrates how to structure complex research queries and configure the agent for optimal performance. The function includes parameterization for research scope and provides structured output formatting."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "id": "00eb46f3",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "üîç Starting research on: Impact of artificial intelligence on job markets in 2025\n",
                  "============================================================\n",
                  "\n",
                  "üìä Research Complete!\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "async def research_assistant(query: str, max_sources: int = 5):\n",
            "    \"\"\"\n",
            "    A comprehensive research assistant using our web scraping agent\n",
            "    \n",
            "    Args:\n",
            "        query (str): The research question or topic\n",
            "        max_sources (int): Maximum number of sources to analyze\n",
            "    \"\"\"\n",
            "    \n",
            "    print(f\"üîç Starting research on: {query}\")\n",
            "    print(\"=\"*60)\n",
            "    \n",
            "    # Enhanced research prompt\n",
            "    research_prompt = f\"\"\"\n",
            "    Please conduct comprehensive research on: \"{query}\"\n",
            "    \n",
            "    Your research should include:\n",
            "    1. Current news and developments (last 30 days)\n",
            "    2. Expert opinions and analysis\n",
            "    3. Statistical data and trends\n",
            "    4. Social media sentiment (if relevant)\n",
            "    5. Key players and companies involved\n",
            "    \n",
            "    Use multiple sources and provide a well-structured summary with:\n",
            "    - Executive summary\n",
            "    - Key findings\n",
            "    - Supporting data\n",
            "    - Sources used\n",
            "    \n",
            "    Limit your research to {max_sources} high-quality sources.\n",
            "    \"\"\"\n",
            "    \n",
            "    result = await agent.ainvoke({\n",
            "        \"messages\": [(\"human\", research_prompt)]\n",
            "    })\n",
            "    \n",
            "    print(f\"\\nüìä Research Complete!\")\n",
            "    print(result[\"messages\"][-1].content)\n",
            "    \n",
            "    return result\n",
            "\n",
            "# Test the research assistant\n",
            "research_result = await research_assistant(\"Impact of artificial intelligence on job markets in 2025\",5)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "research_assistant_benefits_section",
         "metadata": {},
         "source": [
            "### Research Assistant Benefits and Applications\n",
            "\n",
            "The comprehensive research assistant function demonstrates several key benefits:\n",
            "\n",
            "**Scalable Research Framework**: The parameterized approach allows the same function to handle research projects of varying scope and complexity.\n",
            "\n",
            "**Consistent Output Structure**: The structured prompt ensures that research results follow a predictable format, facilitating integration with other systems or processes.\n",
            "\n",
            "**Quality Focus**: By limiting the number of sources and emphasizing quality, the function prioritizes depth over breadth in research coverage.\n",
            "\n",
            "**Professional Standards**: The research methodology incorporates best practices from professional research organizations, ensuring outputs meet enterprise requirements.\n",
            "\n",
            "**Customization Capability**: The function can be easily modified to address specific industry requirements or research methodologies."
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "agents-towards-production",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
