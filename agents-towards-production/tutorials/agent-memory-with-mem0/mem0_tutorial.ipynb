{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiCF4LhgMCo0"
      },
      "source": [
        "![](https://europe-west1-atp-views-tracker.cloudfunctions.net/working-analytics?notebook=tutorials--agent-memory-with-mem0--mem0-tutorial)\n",
        "\n",
        "# Persistent Memory for AI Agents with Mem0\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NirDiamant/agents-towards-production/blob/main/tutorials/agent-memory-with-mem0/mem0_tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DfpNPdeLvV6"
      },
      "source": [
        "## üéØ Introduction\n",
        "\n",
        "AI agents suffer from a fundamental limitation:¬†**memory amnesia**. They forget everything after each conversation, can't learn from past interactions, and lose valuable context that could make them truly intelligent assistants.\n",
        "\n",
        "While basic memory solutions store static information,¬†**[Mem0](https://mem0.dev/github/nir)**¬†introduces a revolutionary approach:¬†**self-improving memory**¬†that automatically extracts insights, resolves conflicts, and evolves with each interaction. Instead of just storing conversations, Mem0 builds an intelligent knowledge system that learns user preferences and provides contextual understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceXLcDIhM0nL"
      },
      "source": [
        "## üõ†Ô∏è What We'll Build\n",
        "\n",
        "We'll create an intelligent Personal AI Research Assistant that demonstrates Mem0's full capabilities:\n",
        "\n",
        "It will:\n",
        "\n",
        "*   Keep track of your preferences for depth, style, and formatting\n",
        "*   Store research notes, summaries, and insights in vector memory for semantic recall\n",
        "*   Capture key ideas from your notes and link related concepts inside a graph database\n",
        "*   Recognize when new questions relate to earlier discussions and surface the right context\n",
        "*   Use both similarity search and structured relationships to generate more informed answers over time\n",
        "\n",
        "Instead of restarting from zero every session, the assistant continually builds on what it already knows - helping you form a growing, interconnected understanding of the topics you research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zZ4-sE0MrWF"
      },
      "source": [
        "## üèóÔ∏è Memory Architecture Overview\n",
        "\n",
        "**Mem0** is a memory layer for AI agents that automatically extracts important information from conversations, stores it in vector and graph databases, and retrieves it later to provide long-term context and continuity.\n",
        "\n",
        "Below is the high-level flow illustrating how Mem0 processes and stores memory.\n",
        "\n",
        "![mem0_architecture](Assets/mem0_architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPlA2sdYNROQ"
      },
      "source": [
        "## üé¢ Learning Journey\n",
        "\n",
        "**Phase 1: Vector Memory Foundation**\n",
        "\n",
        "*   Basic Assistant: Build a working research assistant with vector memory\n",
        "*   Memory Operations: See knowledge extraction and conflict resolution in action\n",
        "*   Semantic Search: Experience intelligent retrieval beyond keywords\n",
        "*   Self-Improvement: Watch memory evolve with each interaction\n",
        "\n",
        "**Phase 2: Graph Enhancement**\n",
        "\n",
        "*   Graph Capabilities: Add entity relationship mapping\n",
        "*   Research Networks: Track paper citations, author connections\n",
        "*   Knowledge Graphs: Visualize research domain relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZo15HErLyJN"
      },
      "source": [
        "## ‚öôÔ∏è Prerequisites and Environment Setup\n",
        "\n",
        "For this tutorial, you'll need:\n",
        "\n",
        "1. [OpenAI API Key](https://platform.openai.com/signup) - used for LLM processing (GPT-4) and embeddings\n",
        "2. [Qdrant Cloud Account](https://cloud.qdrant.io) or any other vector store supported by Mem0 for vector memory.\n",
        "3. [Neo4j AuraDB](https://neo4j.com/aura/) or any other graph database supported by Mem0 for Graph Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uBIhe6JNmTs"
      },
      "source": [
        "### Installation\n",
        "\n",
        "Let's start by installing all required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CjVBRRwNnSu"
      },
      "outputs": [],
      "source": [
        "!pip install -q mem0ai openai python-dotenv neo4j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B8PHfA7NpQ6"
      },
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMZfln_dNr4o"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Mem0 and OpenAI\n",
        "from mem0 import Memory\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ2R5yWxNvlo"
      },
      "source": [
        "### Required API Keys\n",
        "\n",
        "Let's configure your API keys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMl0twLRNx0-"
      },
      "outputs": [],
      "source": [
        "def setup_api_keys():\n",
        "\n",
        "    # OpenAI API Key\n",
        "    if \"OPENAI_API_KEY\" not in os.environ:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "    # Qdrant Cloud (Required for Phase 1: Vector Memory)\n",
        "\n",
        "    if \"QDRANT_URL\" not in os.environ:\n",
        "        os.environ[\"QDRANT_URL\"] = input(\"\\nEnter your Qdrant Cloud URL: \")\n",
        "    if \"QDRANT_API_KEY\" not in os.environ:\n",
        "        os.environ[\"QDRANT_API_KEY\"] = getpass.getpass(\"Enter your Qdrant API key: \")\n",
        "\n",
        "    return True\n",
        "\n",
        "# Run the setup\n",
        "use_qdrant_cloud = setup_api_keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB4mmiUYNzrf"
      },
      "source": [
        "## Memory Configuration\n",
        "## Phase 1: Vector Memory\n",
        "\n",
        "Now let's configure Mem0's memory system. We're starting with¬†**vector-based memory**¬†- the foundation that makes everything else possible.\n",
        "\n",
        "\n",
        "### Understanding the Configuration\n",
        "\n",
        "Each component serves a specific purpose in the memory pipeline:\n",
        "\n",
        "- **ü§ñ LLM Provider**: Extracts insights and resolves conflicts\n",
        "- **üìä Embedder**: Converts text into semantic vectors\n",
        "- **üóÑÔ∏è Vector Store**: Stores embeddings for similarity search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLZ_Yc9WN2ou"
      },
      "outputs": [],
      "source": [
        "if use_qdrant_cloud:\n",
        "    # Enhanced configuration with Qdrant Cloud\n",
        "    config = {\n",
        "    \"llm\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"config\": {\n",
        "            \"model\": \"gpt-4o-mini\",\n",
        "            \"temperature\": 0.1,\n",
        "            \"max_tokens\": 2000,\n",
        "        }\n",
        "    },\n",
        "    \"embedder\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"config\": {\n",
        "            \"model\": \"text-embedding-3-large\",\n",
        "            \"embedding_dims\": 3072,\n",
        "        }\n",
        "    },\n",
        "    \"vector_store\": {\n",
        "        \"provider\": \"qdrant\",\n",
        "        \"config\": {\n",
        "            \"url\": os.environ[\"QDRANT_URL\"],\n",
        "            \"api_key\": os.environ[\"QDRANT_API_KEY\"],\n",
        "            \"collection_name\": \"research_assistant_vectors\",\n",
        "            \"embedding_model_dims\": 3072,  # Must match embedding model\n",
        "        }\n",
        "    },\n",
        "    \"version\": \"v1.1\",  # Latest version with enhanced features\n",
        "}\n",
        "\n",
        "print(\"Vector Memory Configuration Created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht82o6I_N5TL"
      },
      "source": [
        "### Initialize Memory System\n",
        "\n",
        "Now let's create our memory instance - the brain of our research assistant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSlIsPJKN7Xj"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    memory = Memory.from_config(config)\n",
        "    print(\"Memory system initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing memory: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhoYMagaOBry"
      },
      "source": [
        "## Building the Research Assistant\n",
        "\n",
        "Now we'll build our Personal Research Assistant using Mem0's vector memory capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9YlmNjeZlv3"
      },
      "source": [
        "### Core Mem0 Operations\n",
        "\n",
        "Before we build the assistant, let's understand Mem0's main operations:\n",
        "\n",
        "`memory.add()` - Stores new information\n",
        "- Automatically extracts key facts from conversations\n",
        "- Handles deduplication and conflicts\n",
        "- Stores memories with user-specific isolation\n",
        "\n",
        "![mem0_add_architecture](Assets/mem0_add_architecture.jpg)\n",
        "\n",
        "`memory.search()` - Retrieves relevant memories  \n",
        "- Finds semantically similar content (not just keyword matching)\n",
        "- Returns ranked results by relevance\n",
        "- Supports filtering by user_id\n",
        "\n",
        "![mem0_search_architecture](Assets/mem0_search_architecture.jpg)\n",
        "\n",
        "**Other operations** (not used in this tutorial):\n",
        "- `memory.update()` - Modify existing memories\n",
        "- `memory.delete()` - Remove specific memories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs3B8bUMOCN3"
      },
      "outputs": [],
      "source": [
        "class PersonalResearchAssistant:\n",
        "\n",
        "    def __init__(self, memory_instance):\n",
        "        self.client = OpenAI()\n",
        "        self.memory = memory_instance  # Use the configured memory instance\n",
        "        print(f\"Research Assistant initialized with Mem0 memory!\")\n",
        "\n",
        "    def ask(self, question, user_id):\n",
        "\n",
        "        # Search for relevant memories\n",
        "        previous_memories = self.search_memories(question, user_id=user_id)\n",
        "\n",
        "        # Build the prompt with memory context\n",
        "        system_message = \"You are a personal AI Research Assistant. Help users with research questions, remember their interests, and provide contextual recommendations.\"\n",
        "\n",
        "        if previous_memories:\n",
        "            memory_context = \", \".join(previous_memories)\n",
        "            prompt = f\"{system_message}\\n\\nUser input: {question}\\nPrevious memories: {memory_context}\"\n",
        "        else:\n",
        "            prompt = f\"{system_message}\\n\\nUser input: {question}\"\n",
        "\n",
        "        # Generate response\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": prompt},\n",
        "                ],\n",
        "                temperature=0.1,\n",
        "                max_tokens=2000\n",
        "            )\n",
        "\n",
        "            answer = response.choices[0].message.content\n",
        "\n",
        "            # Store the question in memory\n",
        "            self.memory.add(question, user_id=user_id, metadata={\"category\": \"research\"})\n",
        "\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Encountered an error: {e}\"\n",
        "\n",
        "    def get_memories(self, user_id):\n",
        "        try:\n",
        "            memories = self.memory.get_all(user_id=user_id)\n",
        "            if isinstance(memories, dict) and 'results' in memories:\n",
        "                return [m['memory'] for m in memories['results']]\n",
        "            elif isinstance(memories, list):\n",
        "                return [m['memory'] for m in memories]\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error retrieving memories: {e}\")\n",
        "            return []\n",
        "\n",
        "    def search_memories(self, query, user_id):\n",
        "        try:\n",
        "            memories = self.memory.search(query, user_id=user_id)\n",
        "            if isinstance(memories, dict) and 'results' in memories:\n",
        "                return [m['memory'] for m in memories['results']]\n",
        "            elif isinstance(memories, list):\n",
        "                return [m['memory'] for m in memories]\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"Error searching memories: {e}\")\n",
        "            return []\n",
        "\n",
        "# Initialize our research assistant\n",
        "assistant = PersonalResearchAssistant(memory)\n",
        "print(\"\\n Research Assistant ready to learn and help!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqJyNtifOEji"
      },
      "source": [
        "### Test 1: First Interaction - Knowledge Extraction\n",
        "\n",
        "Let's start with a basic research interest:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9lX0-N3OGxH"
      },
      "outputs": [],
      "source": [
        "response1 = assistant.ask(\n",
        "    \"I'm interested in transformer architectures for natural language processing. \"\n",
        "    \"Can you help me find recent papers on this topic?\",\n",
        "    user_id=\"researcher\"\n",
        ")\n",
        "\n",
        "print(f\"Assistant: {response1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ag3SaDsOJBi"
      },
      "source": [
        "### Test 2: Building Context - Watch Memory Connect Ideas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vtzf9OrKOK9P"
      },
      "outputs": [],
      "source": [
        "response2 = assistant.ask(\n",
        "    \"I prefer papers that include practical implementation details and code examples. \"\n",
        "    \"Theoretical papers without code are less useful for my work.\",\n",
        "    user_id=\"researcher\"\n",
        ")\n",
        "\n",
        "print(f\"Assistant: {response2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU1VUjctOMpY"
      },
      "source": [
        "### Test 3: Semantic Search - Intelligence Beyond Keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NKUtwoJOOv1"
      },
      "outputs": [],
      "source": [
        "response3 = assistant.ask(\n",
        "    \"What about BERT and GPT models? Are they related to my research interests?\",\n",
        "    user_id=\"researcher\"\n",
        ")\n",
        "\n",
        "print(f\"Assistant: {response3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev3-w7UxOfbZ"
      },
      "source": [
        "### Test 4: Conflict Resolution - Intelligent Memory Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJw6FDV1OmfY"
      },
      "outputs": [],
      "source": [
        "response4 = assistant.ask(\n",
        "    \"Actually, I also need to understand the theoretical foundations of attention mechanisms. \"\n",
        "    \"Can you recommend some foundational theory papers?\",\n",
        "    user_id=\"researcher\"\n",
        ")\n",
        "\n",
        "print(f\"Assistant: {response4}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USh-JIR-OozR"
      },
      "source": [
        "## üìä Memory Analysis - See What Was Learned\n",
        "\n",
        "Let's examine what our assistant learned from these interactions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCCVOfShOr9P"
      },
      "outputs": [],
      "source": [
        "def analyze_extracted_memories():\n",
        "\n",
        "    print(\"üìà MEMORY ANALYSIS - What Did the Assistant Learn?\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Get all memories\n",
        "    all_memories = assistant.get_memories(user_id=\"researcher\")\n",
        "\n",
        "    if all_memories:\n",
        "        print(f\"\\nüß† Total memories extracted: {len(all_memories)}\")\n",
        "        print(f\"\\nüìö Key insights Mem0 learned about you:\")\n",
        "\n",
        "        for i, memory in enumerate(all_memories, 1):\n",
        "            print(f\"\\n{i}. {memory}\")\n",
        "\n",
        "        # Test semantic search\n",
        "        print(f\"\\nüîç Testing Semantic Search Capabilities:\")\n",
        "\n",
        "        test_queries = [\n",
        "            \"neural networks\",           # Should connect to transformers\n",
        "            \"code implementations\",      # Should find practical preferences\n",
        "            \"attention mechanisms\",      # Should connect to transformer interest\n",
        "            \"deep learning papers\"       # Should find research interests\n",
        "        ]\n",
        "\n",
        "        for query in test_queries:\n",
        "            related_memories = assistant.search_memories(query, user_id=\"researcher\")\n",
        "            print(f\"\\n   üîé Query: '{query}'\")\n",
        "            print(f\"      Found {len(related_memories)} related memories\")\n",
        "            if related_memories:\n",
        "                print(f\"      Top match: {related_memories[0][:100]}...\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è No memories found. Try running the interaction tests first.\")\n",
        "\n",
        "    return all_memories\n",
        "\n",
        "# Run the analysis\n",
        "memories = analyze_extracted_memories()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAktx-BcOuTH"
      },
      "source": [
        "## Interactive Demo - Try It Yourself\n",
        "\n",
        "Now it's your turn! Try asking questions and watch the memory system learn about your research interests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIMV7TSyOiff"
      },
      "outputs": [],
      "source": [
        "def interactive_demo():\n",
        "    while True:\n",
        "        try:\n",
        "            question = input(\"\\nYou: \")\n",
        "\n",
        "            if question.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"\\n Thanks for trying the Research Assistant!\")\n",
        "                break\n",
        "            elif question.lower() == 'memories':\n",
        "                memories = assistant.get_memories(user_id=\"researcher\")\n",
        "                if memories:\n",
        "                    print(f\"\\n Here's what I've learned about your research interests:\")\n",
        "                    for i, memory in enumerate(memories, 1):\n",
        "                        print(f\"   {i}. {memory}\")\n",
        "                else:\n",
        "                    print(\"\\n No memories yet. Start asking about your research interests!\")\n",
        "            elif question.strip():\n",
        "                response = assistant.ask(question)\n",
        "                print(f\"\\n Assistant: {response}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\n Demo ended. Thanks for trying the Research Assistant!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\n Error: {e}\")\n",
        "\n",
        "# Run the interactive demo\n",
        "interactive_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eveKsLcGO267"
      },
      "source": [
        "## üöÄ Phase 2: Enhancing with Graph Memory Capabilities\n",
        "\n",
        "Now that you've mastered **vector-based memory** and seen its power, let's enhance our system with **graph capabilities**. This addition will enable explicit relationship mapping between entities, concepts, and research domains.\n",
        "\n",
        "### Why Add Graph Memory?\n",
        "\n",
        "**Vector memory excels at**:\n",
        "\n",
        "- Finding semantically similar content\n",
        "- \"Show me papers like this one\"\n",
        "- Understanding conceptual similarity\n",
        "\n",
        "**Graph memory adds**:\n",
        "\n",
        "- Explicit entity relationships\n",
        "- \"Who collaborated with whom?\"\n",
        "- \"How did Paper A influence Paper B?\"\n",
        "- Research lineage and citation networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ8kukbWO761"
      },
      "source": [
        "### Graph Storage Setup\n",
        "\n",
        "To add graph capabilities, we need a graph database. Neo4j is the most popular choice, you can use other graph storage as well that are supported by Mem0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N20HcnbxQjJK"
      },
      "outputs": [],
      "source": [
        "def setup_graph_capabilities():\n",
        "\n",
        "    print(\"\\nüîó Enter your Neo4j connection details:\")\n",
        "    neo4j_uri = input(\"Neo4j URI (e.g., neo4j+s://xxx.databases.neo4j.io): \")\n",
        "    neo4j_username = input(\"Username (usually 'neo4j'): \")\n",
        "    neo4j_password = getpass.getpass(\"Password: \")\n",
        "\n",
        "    print(\"\\n Neo4j configuration complete!\")\n",
        "\n",
        "    return True, {\n",
        "        \"url\": neo4j_uri,\n",
        "        \"username\": neo4j_username,\n",
        "        \"password\": neo4j_password\n",
        "    }\n",
        "\n",
        "# Setup graph capabilities for Phase 2\n",
        "has_graph, graph_config = setup_graph_capabilities()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciRnHrr7Qjq4"
      },
      "source": [
        "### Enhanced Configuration with Graph Storage\n",
        "\n",
        "Now let's create the hybrid configuration with both vector and graph storage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wMyY9KPQnuM"
      },
      "outputs": [],
      "source": [
        "# Enhanced configuration with both vector and graph storage\n",
        "enhanced_config = {\n",
        "    \"llm\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"config\": {\n",
        "            \"model\": \"gpt-4o-mini\",\n",
        "            \"temperature\": 0.1,\n",
        "            \"max_tokens\": 2000,\n",
        "        }\n",
        "    },\n",
        "    \"embedder\": {\n",
        "        \"provider\": \"openai\",\n",
        "        \"config\": {\n",
        "            \"model\": \"text-embedding-3-large\",\n",
        "            \"embedding_dims\": 3072,\n",
        "        }\n",
        "    },\n",
        "    \"vector_store\": {\n",
        "        \"provider\": \"qdrant\",\n",
        "        \"config\": {\n",
        "            \"url\": os.environ[\"QDRANT_URL\"],\n",
        "            \"api_key\": os.environ[\"QDRANT_API_KEY\"],\n",
        "            \"collection_name\": \"research_assistant_hybrid\",\n",
        "            \"embedding_model_dims\": 3072,\n",
        "        }\n",
        "    },\n",
        "    \"graph_store\": {\n",
        "        \"provider\": \"neo4j\",\n",
        "        \"config\": graph_config\n",
        "    },\n",
        "    \"version\": \"v1.1\",\n",
        "}\n",
        "\n",
        "try:\n",
        "    enhanced_memory = Memory.from_config(enhanced_config)\n",
        "    print(\"\\n Hybrid Memory System Initialized Successfully!\")\n",
        "\n",
        "    enhanced_assistant = PersonalResearchAssistant(enhanced_memory)\n",
        "    print(\"\\n Enhanced Research Assistant ready with hybrid memory!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" Error initializing hybrid memory: {e}\")\n",
        "\n",
        "    # Fallback to vector-only\n",
        "    print(\"\\n Falling back to vector-only assistant...\")\n",
        "    enhanced_assistant = assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei_zIn4_QxQW"
      },
      "source": [
        "### üìù Graph-Enhanced Research Examples\n",
        "\n",
        "If you have graph storage enabled, let's see it in action with entity relationships:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZEHZAWBQzz-"
      },
      "source": [
        "### Example 1: Research lineage mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0Lkx2CnQ4Ek"
      },
      "outputs": [],
      "source": [
        "graph_response1 = enhanced_assistant.ask(\n",
        "    \"I'm studying the lineage of transformer papers. The original 'Attention Is All You Need' \"\n",
        "    \"by Vaswani et al. led to BERT by Devlin et al., and then to many other models. \"\n",
        "    \"Can you help me map these research connections and suggest related work?\",\n",
        "    user_id=\"graph_user\"\n",
        ")\n",
        "\n",
        "print(f\"Hybrid Assistant: {graph_response1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4sdw-HMQm-r"
      },
      "source": [
        "### Example 2: Collaboration network analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu6TedjzQ8j-"
      },
      "outputs": [],
      "source": [
        "graph_response2 = enhanced_assistant.ask(\n",
        "    \"What other researchers have worked on transformer architectures? \"\n",
        "    \"I want to understand the collaboration network and research groups in this field.\",\n",
        "    user_id=\"graph_user\"\n",
        ")\n",
        "\n",
        "print(f\"Hybrid Assistant: {graph_response2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyANXw3SO5BD"
      },
      "source": [
        "## üéì Recap\n",
        "\n",
        "You just built a real memory system from the ground up. Here's what you accomplished and where to go from here.\n",
        "\n",
        "### What you built\n",
        "- **Hybrid memory architecture** with vector search (Qdrant) + graph relationships (Neo4j)\n",
        "- **Automatic memory extraction** that pulls key facts from conversations  \n",
        "- **Conflict resolution** that handles contradictions and keeps memories current\n",
        "- **Semantic search** that finds relevant info by meaning, not just keywords\n",
        "\n",
        "### Why Mem0\n",
        "\n",
        "Memory is hard to build right. You'd need months to handle fact extraction, conflict resolution, and semantic search properly. Mem0 gives you battle-tested memory that just works.\n",
        "\n",
        "**What you get:**\n",
        "- Smart memory extraction that pulls key facts from conversations automatically\n",
        "- Conflict handling when users change their minds or contradict themselves  \n",
        "- Semantic search that finds relevant stuff by meaning, not just keywords\n",
        "- Proven performance: 26% better accuracy, 91% faster responses, 90% fewer tokens (Mem0 research paper)\n",
        "\n",
        "You can focus on building your app instead of reinventing memory infrastructure.\n",
        "\n",
        "### Your deployment options\n",
        "\n",
        "**Self-hosted (what we built)**\n",
        "- Full control over your stack  \n",
        "- Custom configurations for vector/graph stores\n",
        "- Great for learning, prototyping, and specific requirements\n",
        "\n",
        "**Mem0 Platform**\n",
        "- Managed service starting at free tier (10K memories)\n",
        "- Enterprise features like analytics and graph memory  \n",
        "- Simple API, zero infrastructure headaches\n",
        "- Ship fast and scale without ops overhead\n",
        "\n",
        "\n",
        "### What's next\n",
        "You now have the foundation for AI that actually remembers and learns. Check out:\n",
        "\n",
        "- **[Mem0 Platform](https://mem0.dev/dashboard/nir)** for production deployment\n",
        "- **[Documentation](https://mem0.dev/docs/nir)** for advanced features and integrations\n",
        "- **[Discord Community](https://mem0.dev/discord/nir)** for questions and discussions\n",
        "\n",
        "You're ready to build AI applications that get smarter with every interaction."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
