{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://europe-west1-atp-views-tracker.cloudfunctions.net/working-analytics?notebook=tutorials--agent-with-tavily-web-access--hybrid-agent-tutorial)\n",
    "\n",
    "## 3. Combine Internal Data with Web Data ðŸŒðŸ¤ðŸ“Š\n",
    "\n",
    "Welcome to tutorial 3!\n",
    "\n",
    "\n",
    "In this tutorial, you'll discover how to build a **hybrid agent** that combines real-time web data with your own internal knowledge sources. For enterprises, the true power of agents comes from integrating proprietary internal data with information from the public web.\n",
    "\n",
    "By the end, you'll be able to:\n",
    "\n",
    "- **Integrate** web search results and private documents into a single agent workflow\n",
    "- **Enable** your agent to compare, contrast, and synthesize information from both public and internal sources\n",
    "- **Build** more accurate, up-to-date, and context-aware applications for research, analysis, and decision-making\n",
    "\n",
    "\n",
    "> **Why is this approach valuable?**  \n",
    "> It empowers your AI to answer questions with the most current information available online, while also leveraging your proprietary dataâ€”just like a human expert would.  \n",
    ">  \n",
    "> You'll unlock new capabilities for market research, competitive intelligence, and any scenario where both internal and external knowledge matter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Follow these steps to set up:\n",
    "\n",
    "1. **Sign up** for Tavily at [app.tavily.com](https://app.tavily.com/home/?utm_source=github&utm_medium=referral&utm_campaign=nir_diamant) to get your API key.\n",
    "\n",
    "   *Refer to the screenshots linked below for step-by-step guidance:*\n",
    "\n",
    "   - [Screenshot: Signup Page](assets/sign-up.png)\n",
    "   - [Screenshot: Tavily API Keys Dashboard](assets/api-key.png)\n",
    "\n",
    "\n",
    "2. **Sign up** for OpenAI to get your API key. By default, weâ€™ll use OpenAIâ€”but you can substitute any other LLM provider.\n",
    "   \n",
    "\n",
    "2. **Copy your API keys** from your Tavily and OpenAI account dashboard.\n",
    "\n",
    "3. **Paste your API keys** into the cell below and execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To export your API keys into a .env file, run the following cell (replace with your actual keys):\n",
    "!echo \"TAVILY_API_KEY=<your-tavily-api-key>\" >> .env\n",
    "!echo \"OPENAI_API_KEY=<your-openai-api-key>\" >> .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U tavily-python langchain-chroma langchain-openai langgraph langchain-tavily --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Your Tavily API Client\n",
    "\n",
    "The code below will instantiate the Tavily client with your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Prompt the user to securely input the API key if not already set in the environment\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY:\\n\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "# Initialize the Tavily API client using the loaded or provided API key\n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database Set Up\n",
    "\n",
    "In this tutorial, weâ€™ll use a pre-built vector database to simulate an internal knowledge store. The data is based on synthetically generated mock CRM documents, designed to support a CRM and sales insights use case.\n",
    "\n",
    "To keep things simple, we've prepared a vector store using Chroma and OpenAI embeddings, built from the mock CRM data. Feel free to explore the underlying documents in the [supplemental/docs](supplemental/docs) directory.\n",
    "\n",
    "To get started, simply initialize the vector index by running the Python cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vector store\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize the embeddings model with the default model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize the vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"crm\",  # replace with \"my_custom_index\" for custom documents option\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"supplemental/db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you'd prefer to integrate your own documents, we've made it easy to vectorize your files by following three steps:\n",
    "1. Upload your files (must be PDF) to the [supplemental/docs](supplemental/docs) directory.\n",
    "\n",
    "2. Follow the [`vectorize_tutorial.ipynb`](supplemental/vectorize_tutorial.ipynb) notebook to create your custom vector index.\n",
    "\n",
    "3. Replace `collection_name=\"crm\"` with `collection_name=\"my_custom_index\"` in the Chroma instantiation code in the cell above and re-run it.\n",
    "\n",
    "Note: the vector database pipeline we created is just for demonstration and not optimized â€” youâ€™re encouraged to plug in your own optimized vector pipeline, vector store, and embeddings model to suit your data and needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our vector index with a simple query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Query the vector index\n",
    "search_results = retriever.invoke(\"robotics use case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the results of the vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "for result in search_results:\n",
    "    print(result.page_content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up our tools: both the web tools introduced in Tutorial 2 and our new vector search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the set of tools our agent will use\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_tavily import TavilyExtract\n",
    "from langchain_tavily import TavilyCrawl\n",
    "\n",
    "# Define the vector search tool\n",
    "vector_search_tool = retriever.as_tool(\n",
    "    name=\"vector_search\",\n",
    "    description=\"\"\"\n",
    "    Perform a vector search on our company's CRM data.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Define the LangChain search tool\n",
    "search = TavilySearch(max_results=10, topic=\"general\")\n",
    "\n",
    "# Define the LangChain extract tool\n",
    "extract = TavilyExtract(extract_depth=\"advanced\")\n",
    "\n",
    "# Define the LangChain crawl tool\n",
    "crawl = TavilyCrawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up several OpenAI foundation models, such as o3-mini and the gpt-4.1 model. If you prefer a different LLM provider, you can easily plug in any LangChain Chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the OpenAI foundation models\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# o3-mini-2025-01-31\n",
    "o3_mini = ChatOpenAI(model=\"o3-mini-2025-01-31\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# gpt-4.1\n",
    "gpt_4_1 = ChatOpenAI(model=\"gpt-4.1\", api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define our hybrid react agent using LangGraph, illustrated in the diagram below. \n",
    "\n",
    "<img src=\"./assets/hybrid.svg\" alt=\"Agent\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system prompt is a crucial component in guiding the agent's behavior, as it sets the context, boundaries, and instructions for how the agent should interact with available tools and data sources. A well-crafted system prompt ensures the agent understands what information it can access, how to use each tool effectively, and what standards to follow when generating responses.\n",
    "\n",
    "The system prompt below explicitly outlines the agent's capabilities, including access to both public web tools (search, extract, crawl) and an internal vector search tool for proprietary CRM data. It details the purpose and best practices for each tool, emphasizes the importance of grounding answers in retrieved information, and prohibits the agent from making up facts or relying on prior knowledge. This comprehensive prompt helps the agent make informed decisions about when and how to use each tool, resulting in more accurate and reliable outputs. \n",
    "\n",
    "The prompt clearly describes what information is available in the vector store, so the agent knows how to \n",
    "use it alongside the web tools. Feel free to experiment with the system prompt and try different language models to adjust the \n",
    "agent's behavior and optimize results for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import datetime\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "today = datetime.datetime.today().strftime(\"%A, %B %d, %Y\")\n",
    "\n",
    "# Create the hybrid agent\n",
    "hybrid_agent = create_react_agent(\n",
    "    model=gpt_4_1,\n",
    "    tools=[search, crawl, extract, vector_search_tool],\n",
    "    prompt=ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                f\"\"\"\n",
    "        You are a ReAct-style research agent equipped with the following tools:\n",
    "\n",
    "        * **Tavily Web Search**\n",
    "        * **Tavily Web Extract**\n",
    "        * **Tavily Web Crawl**\n",
    "        * **Internal Vector Search** (for proprietary CRM data)\n",
    "\n",
    "        Your mission is to conduct comprehensive, accurate, and up-to-date research, using a combination of real-time public web information and internal enterprise knowledge.  All answers must be grounded in retrieved information from the tools provided. You are not permitted to make up information or rely on prior knowledge.\n",
    "\n",
    "        **Today's Date:** {today}\n",
    "\n",
    "        **Available Tools:**\n",
    "\n",
    "        1. **Tavily Web Search**\n",
    "\n",
    "        * **Purpose:** Retrieve relevant web pages based on a query.\n",
    "        * **Usage:** Provide a search query to receive up to 10 semantically ranked results, each containing the title, URL, and a content snippet.\n",
    "        * **Best Practices:**\n",
    "            * Use specific queries to narrow down results.\n",
    "                * Example: \"OpenAI's latest product release\" and \"OpenAI's headquarters location\" rather than \"OpenAI's latest product release and headquarters location\"\n",
    "            * Optimize searches using parameters such as `search_depth`, `time_range`, `include_domains`, and `include_raw_content`.\n",
    "            * Break down complex queries into specific, focused sub-queries.\n",
    "\n",
    "        2. **Tavily Web Crawl**\n",
    "\n",
    "        * **Purpose:** Explore a website's structure and gather content from linked pages for deep research and information discovery from a single source.\n",
    "        * **Usage:** Input a base URL to crawl, specifying parameters such as `max_depth`, `max_breadth`, and `extract_depth`.\n",
    "        * **Best Practices:**\n",
    "\n",
    "            * Begin with shallow crawls and progressively increase depth.\n",
    "            * Utilize `select_paths` or `exclude_paths` to focus the crawl.\n",
    "            * Set `extract_depth` to \"advanced\" for comprehensive extraction.\n",
    "\n",
    "        3. **Tavily Web Extract**\n",
    "\n",
    "        * **Purpose:** Extract the full content from specific web pages.\n",
    "        * **Usage:** Provide URLs to retrieve detailed content.\n",
    "        * **Best Practices:**\n",
    "\n",
    "            * Set `extract_depth` to \"advanced\" for detailed content, including tables and embedded media.\n",
    "            * Enable `include_images` if image data is necessary.\n",
    "\n",
    "        4. **Internal Vector Search**\n",
    "\n",
    "        * **Purpose:** Search the proprietary CRM knowledge base.\n",
    "        * **Usage:** Submit a natural language query to retrieve relevant context from the CRM vector store. Contains context about key accounts like Meta, Apple, Google, Amazon, Microsoft, and Tesla.\n",
    "        * **Best Practices:**\n",
    "            * When possible, refer to specific information, such as names, dates, product usage, or meetings.\n",
    "            \n",
    "\n",
    "        **Guidelines for Conducting Research:**\n",
    "\n",
    "        * You may not answer questions based on prior knowledge or assumptions.\n",
    "        * **Citations:** Always support findings with source URLs, clearly provided as in-text citations.\n",
    "        * **Accuracy:** Rely solely on data obtained via provided tools (web or vector store) â€”never fabricate information.\n",
    "        * **If none of the tools return useful information, respond:**\n",
    "            * \"I'm sorry, but none of the available tools provided sufficient information to answer this question.\"\n",
    "\n",
    "        **Research Workflow:**\n",
    "\n",
    "        * **Thought:** Consider necessary information and next steps.\n",
    "        * **Action:** Select and execute appropriate tools.\n",
    "        * **Observation:** Analyze obtained results.\n",
    "        **IMPORTANT: Repeat Thought/Action/Observation cycles as needed. You MUST only respond to the user once you have gathered all the information you need.**\n",
    "        * **Final Answer:** Synthesize and present findings with citations in markdown format.\n",
    "        \n",
    "        **Example Workflow:**\n",
    "\n",
    "        **Workflow 4: Combine Web Tools and Vector Search**\n",
    "\n",
    "        **Question:** What has Apple publicly shared about their AI strategy, and do we have any internal notes on recent meetings with them?\n",
    "\n",
    "        * **Thought:** Iâ€™ll start by looking for Appleâ€™s public statements on AI using a search.\n",
    "        * **Action:** Tavily Web Search with the query \"Apple AI strategy 2024\" and `time_range` set to \"month\".\n",
    "        * **Observation:** Found a recent article from Appleâ€™s newsroom and a keynote transcript.\n",
    "        * **Thought:** I want to read the full content of Appleâ€™s keynote.\n",
    "        * **Action:** Tavily Web Extract on the URL from the search result.\n",
    "        * **Observation:** Extracted detailed content from Appleâ€™s announcement.\n",
    "        * **Thought:** Now, Iâ€™ll check our internal CRM data for recent meeting notes with Apple.\n",
    "        * **Action:** Internal Vector Search with the query \"recent Apple meetings AI strategy\".\n",
    "        * **Observation:** Retrieved notes from a Q2 strategy sync with Appleâ€™s enterprise team.\n",
    "        * **Final Answer:** Synthesized response combining public and internal data, with citations.\n",
    "        ---\n",
    "\n",
    "        You will now receive a research question from the user:\n",
    "        \"\"\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    ),\n",
    "    name=\"hybrid_agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Test the hybrid agent\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"Search for the latest news on google relevant to our current CRM data on them\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Stream the agent's response\n",
    "for s in hybrid_agent.stream(inputs, stream_mode=\"values\"):\n",
    "    message = s[\"messages\"][-1]\n",
    "    if isinstance(message, tuple):\n",
    "        print(message)\n",
    "    else:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the intermediary agent traces above to see how the agent combined the vector search and web search together.\n",
    "\n",
    "Let's view the final output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final output as markdown\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the final report integrates relevant web insights with our existing CRM data on Google Cloud ML Ops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another example. We'll ask the agent: \"Check for the Google deal size and find the latest earnings report for them to determine if they are currently on a spending spree.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the hybrid agent\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"Check for the Google Deal size and find the latest earnings report for them to validate if they are in spending spree\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Stream the agent's response\n",
    "for s in hybrid_agent.stream(inputs, stream_mode=\"values\"):\n",
    "    message = s[\"messages\"][-1]\n",
    "    if isinstance(message, tuple):\n",
    "        print(message)\n",
    "    else:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final output as markdown\n",
    "Markdown(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    " \n",
    "Congratulations on completing this tutorialâ€”and the entire seriesâ€”on building intelligent agents with web access using the Tavily API!\n",
    " \n",
    "Throughout these tutorials, you've learned how to:\n",
    " \n",
    "- **Set up the Tavily API** for real-time web search, extraction, and crawling.\n",
    "- **Design agent architectures** that combine both web and internal knowledge sources.\n",
    "- **Integrate multiple tools**â€”including web search, extract, crawling, and vector searchâ€”into a unified, powerful agent system.\n",
    "- **Apply these agents** to real-world scenarios, such as researching companies and analyzing news.\n",
    " \n",
    "By leveraging Tavily's web API, your agents can now access up-to-date information beyond their training data, making them more effective and relevant for dynamic business.\n",
    " \n",
    "Well done on reaching the end of this series! We hope these skills empower you to build even more advanced and insightful AI solutions. Happy coding!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
